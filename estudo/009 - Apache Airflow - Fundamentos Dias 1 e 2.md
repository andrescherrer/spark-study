# Apache Airflow - Fundamentos Completos

Com 20 anos na √°rea, posso dizer que Airflow revolucionou como orquestramos pipelines de dados. Vou desdobrar isso para voc√™ de forma pr√°tica.

## 1Ô∏è‚É£ DAG, TASK, OPERATOR - O Trip√© Fundamental

### DAG (Directed Acyclic Graph)

√â o "projeto" de execu√ß√£o. Imagine como um blueprint de um pr√©dio - define quais tarefas existem e como se relacionam. √â ac√≠clico porque n√£o pode ter loops (A ‚Üí B ‚Üí A √© proibido).

```
Meu_Pipeline_Di√°rio (DAG)
    ‚îú‚îÄ‚îÄ Extrair_Dados (Task)
    ‚îú‚îÄ‚îÄ Validar_Dados (Task)
    ‚îî‚îÄ‚îÄ Carregar_Dados (Task)
```

### Task

√â a unidade de trabalho. Cada ret√¢ngulo no diagrama acima √© uma Task.

### Operator

√â a implementa√ß√£o da Task. Ele define *como* executar. Exemplos:
- `PythonOperator`: executa c√≥digo Python
- `BashOperator`: executa comando bash
- `PostgresOperator`: executa SQL
- `HttpOperator`: faz requisi√ß√µes HTTP

### üìö Fontes Oficiais

- https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/dags.html
- https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/tasks.html
- https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/operators.html

---

## 2Ô∏è‚É£ SCHEDULING E TRIGGERS

### Scheduling

Determina *quando* sua DAG roda. Usa cron-like expressions:

```python
from airflow import DAG
from datetime import datetime, timedelta

dag = DAG(
    'meu_pipeline',
    default_args={
        'owner': 'data_team',
        'retries': 2,
        'retry_delay': timedelta(minutes=5)
    },
    schedule_interval='0 2 * * *',  # 2 AM todo dia
    start_date=datetime(2025, 1, 1),
    catchup=False
)
```

- `0 2 * * *` = Todo dia √†s 2 da manh√£
- `0 */6 * * *` = A cada 6 horas
- `@daily` = Atalho para diariamente
- `None` = Manual apenas

### Triggers

S√£o condi√ß√µes que ativam Tasks:
- **Time-based**: Agenda fixa
- **Sensor**: Espera algo acontecer (arquivo chegar, DB atualizar)
- **External**: Webhook, API
- **Conditional**: Resultado de outra task

### üìö Fontes Oficiais

- https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/dags.html#scheduling
- https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/sensors.html

---

## 3Ô∏è‚É£ XCOM E TASK DEPENDENCIES

### XCom (Cross Communication)

Permite que Tasks compartilhem dados pequenos (strings, n√∫meros, dicion√°rios - n√£o arquivos grandes!).

```python
from airflow.operators.python import PythonOperator

def extrair_dados():
    resultado = {"total": 1000, "status": "sucesso"}
    return resultado  # Isso vira XCom automaticamente

def processar_dados(ti):  # ti = Task Instance
    valor = ti.xcom_pull(task_ids='extrair')
    print(f"Recebi: {valor}")

extract_task = PythonOperator(task_id='extrair', python_callable=extrair_dados, dag=dag)
process_task = PythonOperator(task_id='processar', python_callable=processar_dados, dag=dag)

extract_task >> process_task  # Depend√™ncia
```

### Task Dependencies

Define ordem de execu√ß√£o:
- `>>` : A antes de B
- `<<` : B antes de A
- `<<, >>` : Cadeia

```python
# Op√ß√£o 1: Operador
task_a >> task_b >> task_c

# Op√ß√£o 2: Lista
task_a >> [task_b, task_c]  # B e C em paralelo depois de A

# Op√ß√£o 3: Set
[task_a, task_b] >> task_c  # C ap√≥s ambas terminarem
```

### üìö Fontes Oficiais

- https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/xcoms.html
- https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/tasks.html#dependencies

---

## 4Ô∏è‚É£ SENSOR vs OPERATOR

| Aspecto | Sensor | Operator |
|--------|--------|----------|
| **Prop√≥sito** | Espera algo acontecer | Executa a√ß√£o |
| **Tempo de espera** | Pode ficar horas/dias | Executa e termina |
| **Exemplo** | "Arquivo j√° foi criado?" | "Copiar arquivo" |
| **Modo padr√£o** | `poke` (testa periodicamente) ou `reschedule` | Execute uma vez |

### Exemplo Pr√°tico

```python
from airflow.sensors.filesystem import FileSensor
from airflow.operators.bash import BashOperator

# Sensor: Espera arquivo existir
aguardar_arquivo = FileSensor(
    task_id='aguardar_csv',
    filepath='/data/entrada/vendas.csv',
    poke_interval=30,  # Verifica a cada 30s
    timeout=3600,       # Timeout de 1h
    dag=dag
)

# Operator: Processa o arquivo
processar = BashOperator(
    task_id='processar_csv',
    bash_command='python /scripts/processar.py',
    dag=dag
)

aguardar_arquivo >> processar
```

### üìö Fontes Oficiais

- https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/sensors.html

---

# üè≠ EXEMPLOS DO MUNDO REAL - Engenharia de Dados

## Cen√°rio 1: Pipeline ETL de E-commerce

Imagine uma empresa que precisa atualizar seu data warehouse com vendas do dia anterior:

```python
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.providers.postgres.operators.postgres import PostgresOperator
from airflow.sensors.external_task import ExternalTaskSensor
from datetime import datetime, timedelta
import pandas as pd
import requests

default_args = {
    'owner': 'data_eng',
    'retries': 3,
    'retry_delay': timedelta(minutes=10)
}

dag = DAG(
    'ecommerce_etl_diario',
    default_args=default_args,
    schedule_interval='0 3 * * *',  # 3 AM todo dia
    start_date=datetime(2025, 1, 1)
)

# EXTRACT
def extrair_vendas_api(**context):
    """Busca vendas da API do √∫ltimo dia"""
    data = context['ds']  # Data de execu√ß√£o (YYYY-MM-DD)
    
    response = requests.get(
        f'https://api.ecommerce.com/vendas?data={data}'
    )
    vendas = response.json()
    
    # Salvar localmente temporariamente
    df = pd.DataFrame(vendas)
    df.to_csv(f'/tmp/vendas_{data}.csv', index=False)
    
    # Comunicar quantidade de registros
    return {
        'total_registros': len(vendas),
        'data_arquivo': f'/tmp/vendas_{data}.csv'
    }

extract = PythonOperator(
    task_id='extrair_vendas',
    python_callable=extrair_vendas_api,
    dag=dag
)

# TRANSFORM
def validar_e_transformar(**context):
    """Valida, limpa e enriquece dados"""
    # Pega dados da task anterior
    ti = context['task_instance']
    arquivo = ti.xcom_pull(task_ids='extrair_vendas')['data_arquivo']
    
    df = pd.read_csv(arquivo)
    
    # Valida√ß√µes
    assert df['valor'].dtype in ['float64', 'int64'], "Valor deve ser num√©rico"
    assert len(df) > 0, "Nenhuma venda encontrada"
    
    # Transforma√ß√µes
    df['data_venda'] = pd.to_datetime(df['data_venda'])
    df['margem'] = (df['preco_final'] - df['custo']) / df['preco_final']
    df['categoria_margem'] = pd.cut(
        df['margem'], 
        bins=[0, 0.2, 0.5, 1], 
        labels=['Baixa', 'M√©dia', 'Alta']
    )
    
    # Salvar transformado
    transformado = f'/tmp/vendas_transformadas_{context["ds"]}.parquet'
    df.to_parquet(transformado)
    
    return {
        'registros_validos': len(df),
        'arquivo_final': transformado
    }

transform = PythonOperator(
    task_id='validar_transformar',
    python_callable=validar_e_transformar,
    dag=dag
)

# LOAD
def carregar_warehouse(**context):
    """Insere dados no PostgreSQL"""
    ti = context['task_instance']
    arquivo = ti.xcom_pull(task_ids='validar_transformar')['arquivo_final']
    
    df = pd.read_parquet(arquivo)
    
    # Conex√£o com o warehouse
    from sqlalchemy import create_engine
    engine = create_engine('postgresql://user:pass@warehouse:5432/analytics')
    
    df.to_sql(
        'vendas_staging',
        engine,
        if_exists='append',
        index=False
    )
    
    return f"Carregados {len(df)} registros"

load = PythonOperator(
    task_id='carregar_warehouse',
    python_callable=carregar_warehouse,
    dag=dag
)

# Data quality check
check_quality = PostgresOperator(
    task_id='validar_qualidade_dados',
    sql="""
        SELECT COUNT(*) as total FROM vendas_staging 
        WHERE data_venda = {{ ds }}
        AND margem IS NOT NULL;
    """,
    postgres_conn_id='warehouse',
    dag=dag
)

# Orquestra√ß√£o
extract >> transform >> load >> check_quality
```

### O que acontece:

1. 3 AM: Airflow dispara a DAG
2. Task 1: API retorna 50.000 vendas
3. Task 2: Remove 200 inv√°lidas, deixa 49.800
4. Task 3: Insere no PostgreSQL
5. Task 4: Valida se inseriu corretamente

---

## Cen√°rio 2: Multi-Source com Sensores (Padr√£o real!)

Muitas empresas t√™m dados de m√∫ltiplas fontes. Este padr√£o espera tudo chegar:

```python
from airflow import DAG
from airflow.sensors.filesystem import FileSensor
from airflow.sensors.external_task import ExternalTaskSensor
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta

dag = DAG(
    'multi_source_consolidado',
    schedule_interval='0 6 * * *',  # 6 AM
    start_date=datetime(2025, 1, 1)
)

# SENSORES: Espera dados chegarem
sensor_vendas = FileSensor(
    task_id='aguardar_vendas_s3',
    filepath='s3://data-lake/vendas/{{ ds }}/vendas.parquet',
    poke_interval=60,
    timeout=7200,  # 2 horas para chegar
    dag=dag
)

sensor_clientes = FileSensor(
    task_id='aguardar_clientes_db',
    filepath='/tmp/clientes_{{ ds }}.csv',  # CRM exporta aqui
    poke_interval=30,
    timeout=3600,
    dag=dag
)

# Espera DAG CRM terminar
sensor_external = ExternalTaskSensor(
    task_id='esperar_crm_finalizar',
    external_dag_id='crm_daily_export',
    external_task_id='exportar_clientes',
    allowed_states=['success'],
    failed_states=['failed'],
    poke_interval=60,
    timeout=3600,
    dag=dag
)

# S√≥ depois que tudo chegou, come√ßa a consolida√ß√£o
def consolidar_fontes(**context):
    """Junta dados de 3 fontes"""
    import s3fs
    import pandas as pd
    
    data = context['ds']
    
    # L√™ Parquet do S3
    fs = s3fs.S3FileSystem()
    with fs.open(f's3://data-lake/vendas/{data}/vendas.parquet', 'rb') as f:
        vendas = pd.read_parquet(f)
    
    # L√™ CSV local (CRM)
    clientes = pd.read_csv(f'/tmp/clientes_{data}.csv')
    
    # Join
    consolidado = vendas.merge(clientes, on='cliente_id')
    
    # Salva resultado
    consolidado.to_parquet(f'/data/consolidado_{data}.parquet')

consolidar = PythonOperator(
    task_id='consolidar_dados',
    python_callable=consolidar_fontes,
    dag=dag
)

# Espera TODOS os sensores
[sensor_vendas, sensor_clientes, sensor_external] >> consolidar
```

### Padr√£o real de empresa:

Quando voc√™ tem APIs lentas, SFTPs, databases de diferentes departamentos, precisa coordenar tudo. Sensores mant√™m o Airflow *intelligente*.

---

## Cen√°rio 3: DAG com Retry e Error Handling (Production-ready)

```python
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.providers.slack.operators.slack_webhook import SlackWebhookOperator
from airflow.utils.trigger_rule import TriggerRule
from datetime import datetime, timedelta

dag = DAG(
    'robusto_producao',
    default_args={
        'owner': 'data_eng',
        'retries': 3,
        'retry_delay': timedelta(minutes=5),
        'retry_exponential_backoff': True,  # Aumenta delay exponencialmente
        'max_retry_delay': timedelta(minutes=60)
    },
    schedule_interval='0 2 * * 1-5',  # Seg-Sex √†s 2 AM
    start_date=datetime(2025, 1, 1)
)

def extrair_com_retry(**context):
    """Com tratamento de erro"""
    try:
        # Tenta conex√£o 3x
        for tentativa in range(3):
            try:
                dados = requisitar_api()  # Fun√ß√£o que pode falhar
                return dados
            except ConnectionError:
                if tentativa < 2:
                    time.sleep(2 ** tentativa)  # Backoff
                else:
                    raise
    except Exception as e:
        # Log estruturado
        context['task_instance'].log.error(f"Falha na extra√ß√£o: {str(e)}")
        raise

extract = PythonOperator(
    task_id='extrair',
    python_callable=extrair_com_retry,
    dag=dag
)

# Task que roda mesmo se anterior falhar
notificar_falha = SlackWebhookOperator(
    task_id='notificar_falha',
    http_conn_id='slack_webhook',
    message='Pipeline falhou em {{ task }}',
    trigger_rule=TriggerRule.ONE_FAILED,  # S√≥ roda se houver falha
    dag=dag
)

extract >> notificar_falha
```

---

## üìä Compara√ß√£o com Alternativas (Para Contexto)

| Ferramenta | Use Para | Problema |
|-----------|----------|---------|
| **Airflow** | ETL complexo, depend√™ncias | Curva aprendizado |
| **Cron + Scripts** | Tasks simples | Sem UI, dif√≠cil debugar |
| **Prefect** | Similar ao Airflow, mais novo | Comunidade menor |
| **dbt** | Transforma√ß√µes SQL | N√£o orquestra Python/APIs |
| **Spark Jobs** | Processamento massivo | N√£o √© orquestrador |

**Minha recomenda√ß√£o:** Para Engenharia de Dados hoje, Airflow √© padr√£o de mercado (Uber, Netflix, Airbnb usam). Dominar bem te abre portas.

---

## üìö Roadmap Seu Estudo

Ap√≥s dominar estes 4 pilares (DAG, Task, XCom, Sensors), estude em ordem:

1. **Hooks** - Conex√µes com sistemas externos (DB, S3, APIs)
2. **Connections & Secrets** - Vari√°veis de ambiente, credenciais
3. **Backfill** - Reprocessar dados hist√≥ricos
4. **SLA monitoring** - Alertas se pipeline atrasar
5. **Custom Operators** - Criar seus pr√≥prios operators

Quer que eu detalhe algum destes com exemplos pr√°ticos?