# Especialista em Dados: Projeto Final Integrado - Engenharia de Dados Pleno

**Autor**: Especialista em Dados com 20 anos de experi√™ncia  
**Data**: 2025  
**N√≠vel**: Pleno (Senior Data Engineer)

---

## Sum√°rio
- [Vis√£o Geral](#vis√£o-geral-da-arquitetura)
- [Componentes](#componentes)
- [Exemplos do Mundo Real](#exemplos-do-mundo-real)
- [Fontes de Estudo](#fontes-complementares-para-estudo)
- [Pr√≥ximos Passos](#pr√≥ximos-passos-para-seu-projeto)

---

## 1Ô∏è‚É£ VIS√ÉO GERAL DA ARQUITETURA

A stack que voc√™ vai construir segue o padr√£o **Modern Data Stack (MDS)**:

```
CAMADA 1 (INGEST√ÉO): Dados brutos de m√∫ltiplas fontes
              ‚Üì (normaliza√ß√£o, versionamento)
CAMADA 2 (ORQUESTRA√á√ÉO): Fluxo e agendamento
              ‚Üì (DAGs, depend√™ncias, SLA)
CAMADA 3 (PROCESSAMENTO): Transforma√ß√µes em escala
              ‚Üì (Spark, performance, distribui√ß√£o)
CAMADA 4 (ARMAZENAGEM): Data Lake/Warehouse
              ‚Üì (schema, parti√ß√µes, reten√ß√£o)
CAMADA 5 (TRANSFORMA√á√ÉO): L√≥gica de neg√≥cio
              ‚Üì (DBT, tests, documenta√ß√£o)
CAMADA 6 (QUALIDADE): Valida√ß√£o e compliance
              ‚Üì (Great Expectations, alertas)
CAMADA 7 (CONSUMO): Analytics e BI
```

Este √© o fluxo **E2E (End-to-End)** que voc√™ precisa dominar para Pleno.

### Arquitetura Detalhada

```
[API / Kaggle Dataset]
         ‚Üì
     [S3 / Data Lake]
         ‚Üì
   [Airflow Orchestration]
         ‚Üì
   [Spark Processing]
         ‚Üì
   [Redshift Warehouse]
         ‚Üì
   [Dbt Transformation]
         ‚Üì
[Great Expectations Validation]
         ‚Üì
[Analytics Dashboard]
```

---

## üìã COMPONENTE 1: DATA INGESTION (16h)

### O que voc√™ vai fazer

**Objetivo**: Trazer dados de forma confi√°vel, rastre√°vel e resiliente.

### Conceitos-chave

#### 1. Conectar a Fonte de Verdade
- APIs (REST, GraphQL)
- Bancos de dados (MySQL, PostgreSQL, Oracle)
- Data Warehouses (Snowflake, BigQuery)
- Arquivos (CSV, Parquet, JSON)
- Streams (Kafka, Kinesis)

#### 2. Tratamento de Erros
- Retry logic com backoff exponencial
- Circuit breaker pattern
- Logging e alertas
- Dead letter queues

#### 3. Versionamento do Raw Data
- Manter hist√≥rico completo (nunca deletar)
- Schema evolution
- Audit trail

### Documenta√ß√£o Oficial

- **Apache Airflow - Extractors & Operators**: https://airflow.apache.org/docs/apache-airflow/stable/concepts/operators.html
- **AWS S3 Best Practices**: https://docs.aws.amazon.com/s3/latest/userguide/disaster-recovery-resiliency.html
- **Python Requests Library** (para APIs): https://docs.python-requests.org/
- **SQLAlchemy** (para bancos): https://docs.sqlalchemy.org/

### Exemplo Pr√°tico: Python + Airflow

```python
# DAG simples para ingest√£o
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.utils.dates import days_ago
import boto3
import requests
import json
from datetime import datetime

def extract_from_api(**context):
    """Extrai dados de API e salva em S3"""
    
    # Conectar √† API
    response = requests.get(
        "https://api.dados.com/v1/vendas",
        headers={"Authorization": "Bearer TOKEN"},
        params={"date": context['ds']}  # ds = data de execu√ß√£o do Airflow
    )
    response.raise_for_status()
    
    # Validar resposta
    data = response.json()
    if not isinstance(data, list):
        raise ValueError("Formato esperado: lista de objetos")
    
    # Salvar em S3 com versionamento
    s3 = boto3.client('s3')
    key = f"raw/vendas/dt={context['ds']}/{datetime.now().timestamp()}.json"
    
    s3.put_object(
        Bucket='meu-data-lake',
        Key=key,
        Body=json.dumps(data),
        ServerSideEncryption='AES256'
    )
    
    context['task_instance'].xcom_push(
        key='s3_path',
        value=key
    )

dag = DAG(
    'vendas_ingestao',
    default_args={'owner': 'data-eng', 'retries': 3},
    start_date=days_ago(1),
    schedule_interval='0 2 * * *',  # 02:00 UTC diariamente
    tags=['ingestao']
)

ingest_task = PythonOperator(
    task_id='extract_vendas',
    python_callable=extract_from_api,
    provide_context=True,
    dag=dag,
    pool='api_calls',  # Limite de conex√µes paralelas
    pool_slots=5
)
```

---

## üöÅ COMPONENTE 2: ORCHESTRATION COM AIRFLOW (24h)

### O que voc√™ vai fazer

**Objetivo**: Orquestrar pipelines complexos com confiabilidade de produ√ß√£o.

### Conceitos-chave

#### 1. DAG Completa
- Depend√™ncias entre tasks
- Branching e merging
- Idempot√™ncia

#### 2. Scheduling e SLA
- Cron expressions
- Backfill de dados hist√≥ricos
- SLA alerts

#### 3. Monitoramento e Alertas
- Logs estruturados
- M√©tricas (CPU, mem√≥ria, duration)
- Notifica√ß√µes (Slack, email, PagerDuty)

### Documenta√ß√£o Oficial

- **Apache Airflow Core Concepts**: https://airflow.apache.org/docs/apache-airflow/stable/concepts/overview.html
- **DAG Writing Best Practices**: https://airflow.apache.org/docs/apache-airflow/stable/best-practices.html
- **Airflow Executors** (Local, Celery, Kubernetes): https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/executor/index.html

### Exemplo Pr√°tico: Orquestra√ß√£o Completa

```python
from airflow import DAG
from airflow.operators.bash import BashOperator
from airflow.operators.python import PythonOperator
from airflow.operators.empty import EmptyOperator
from airflow.utils.dates import days_ago
from airflow.models import Variable
from airflow.exceptions import AirflowException
from datetime import datetime, timedelta
import logging

logger = logging.getLogger(__name__)

# Vari√°veis configur√°veis
environment = Variable.get("environment", "prod")
s3_bucket = Variable.get("s3_bucket", "data-lake-prod")

default_args = {
    'owner': 'data-engineering',
    'retries': 3,
    'retry_delay': timedelta(minutes=5),
    'email': ['data-alerts@empresa.com'],
    'email_on_failure': True,
    'email_on_retry': False,
    'start_date': days_ago(2),
    'sla': timedelta(hours=2)  # Pipeline deve terminar em 2h
}

dag = DAG(
    'pipeline_vendas_completo',
    default_args=default_args,
    description='Pipeline E2E de vendas com qualidade',
    schedule_interval='0 2 * * *',  # 02:00 UTC daily
    max_active_runs=1,  # Apenas 1 execu√ß√£o por vez
    catchup=False,
    tags=['vendas', 'producao']
)

def validate_execution_date(**context):
    """Valida√ß√£o antes de iniciar"""
    exec_date = context['execution_date']
    if exec_date.day == 1:
        logger.warning("Primeira execu√ß√£o do m√™s - volume esperado maior")
    logger.info(f"Iniciando pipeline para {exec_date}")

start = EmptyOperator(task_id='start', dag=dag)

validate = PythonOperator(
    task_id='validate_date',
    python_callable=validate_execution_date,
    provide_context=True,
    dag=dag
)

# Ingest√£o paralela de m√∫ltiplas fontes
ingest_mysql = BashOperator(
    task_id='ingest_mysql',
    bash_command=f"""
    python /scripts/ingest_mysql.py \
        --date {{{{ ds }}}} \
        --bucket {s3_bucket}
    """,
    pool='ingestao',
    pool_slots=2,
    dag=dag
)

ingest_api = BashOperator(
    task_id='ingest_api',
    bash_command=f"""
    python /scripts/ingest_api.py \
        --date {{{{ ds }}}} \
        --bucket {s3_bucket}
    """,
    pool='ingestao',
    pool_slots=2,
    dag=dag
)

# Spark processing (aguarda ambas ingest√µes)
process_spark = BashOperator(
    task_id='spark_transform',
    bash_command="""
    spark-submit \
        --master yarn \
        --deploy-mode cluster \
        --num-executors 10 \
        --executor-memory 4g \
        /scripts/spark_processing.py {{ ds }}
    """,
    trigger_rule='all_success',  # S√≥ executa se AMBAS bem-sucedidas
    dag=dag
)

# DBT transformations
dbt_staging = BashOperator(
    task_id='dbt_staging',
    bash_command="""
    cd /dbt && \
    dbt run --models staging.* --select state:modified+ \
        --profiles-dir /dbt/profiles
    """,
    dag=dag
)

dbt_marts = BashOperator(
    task_id='dbt_marts',
    bash_command="""
    cd /dbt && \
    dbt run --models marts.* \
        --profiles-dir /dbt/profiles
    """,
    dag=dag
)

# Data Quality checks
quality_checks = BashOperator(
    task_id='great_expectations',
    bash_command="""
    python /scripts/run_ge_checks.py \
        --checkpoint vendas_checkpoint \
        --date {{ ds }}
    """,
    dag=dag
)

end = EmptyOperator(task_id='end', dag=dag, trigger_rule='all_done')

# Definir depend√™ncias
start >> validate
validate >> [ingest_mysql, ingest_api]
[ingest_mysql, ingest_api] >> process_spark
process_spark >> dbt_staging >> dbt_marts
dbt_marts >> quality_checks >> end
```

### Conceitos Avan√ßados: SLA Definition

```python
sla_miss_callback = lambda context: send_alert(
    f"SLA miss: {context['task'].task_id}",
    channel="data-alerts"
)

default_args['sla_miss_callback'] = sla_miss_callback
```

O pipeline inteiro deve terminar √†s 04:00 UTC, permitindo consumo pelos analistas √†s 06:00.

---

## ‚ö° COMPONENTE 3: PROCESSING COM SPARK (24h)

### O que voc√™ vai fazer

**Objetivo**: Transformar dados em larga escala com performance otimizada.

### Conceitos-chave

#### 1. Transforma√ß√µes Complexas
- Window functions
- Joins eficientes
- Aggrega√ß√µes em escala

#### 2. Performance Otimizada
- Particionamento estrat√©gico
- Broadcast joins
- Caching inteligente

#### 3. Tests Unit√°rios
- Testando transforma√ß√µes
- Data contracts

### Documenta√ß√£o Oficial

- **Apache Spark Documentation**: https://spark.apache.org/docs/latest/
- **Spark SQL Performance Tuning**: https://spark.apache.org/docs/latest/sql-performance-tuning.html
- **PySpark API**: https://spark.apache.org/docs/latest/api/python/index.html

### Exemplo Pr√°tico: Transforma√ß√µes Spark

```python
# spark_processing.py
from pyspark.sql import SparkSession, Window
from pyspark.sql.functions import (
    col, coalesce, row_number, sum as spark_sum,
    lag, datediff, when, broadcast
)
from datetime import datetime
import sys

def create_spark_session():
    """Factory para criar session otimizada"""
    return SparkSession.builder \
        .appName("vendas-processing") \
        .config("spark.sql.shuffle.partitions", "200") \
        .config("spark.sql.adaptive.enabled", "true") \
        .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
        .config("spark.sql.statistics.histogram.enabled", "true") \
        .config("spark.sql.join.preferSortMergeJoin", "true") \
        .config("spark.sql.autoBroadcastJoinThreshold", "100MB") \
        .config("spark.memory.fraction", "0.8") \
        .getOrCreate()

def load_raw_data(spark, date_str):
    """Carrega raw data do S3"""
    path = f"s3a://data-lake-prod/raw/vendas/dt={date_str}/"
    
    df = spark.read \
        .option("badRecordsPath", f"{path}_bad_records") \
        .json(path)
    
    # Valida√ß√£o b√°sica
    assert df.count() > 0, f"Nenhum registro encontrado para {date_str}"
    return df

def transform_vendas(df):
    """Transforma√ß√µes principais"""
    
    # Window function: ranking de clientes por vendas
    window_spec = Window \
        .partitionBy("cliente_id") \
        .orderBy(col("data_venda").desc())
    
    df_ranked = df.withColumn(
        "venda_rank",
        row_number().over(window_spec)
    )
    
    # Agrega√ß√µes: totais por cliente
    df_agg = df_ranked.groupBy("cliente_id") \
        .agg(
            spark_sum("valor_venda").alias("total_vendas"),
            spark_sum("quantidade").alias("qtd_itens"),
            col("cliente_nome").cast("string").alias("cliente_nome")
        ) \
        .filter(col("total_vendas") > 0)
    
    return df_agg

def test_transform():
    """Unit tests para transforma√ß√µes"""
    spark = create_spark_session()
    
    # Test data
    test_data = [
        ("C001", 100.0, 2, "2024-01-15"),
        ("C001", 50.0, 1, "2024-01-14"),
        ("C002", 200.0, 5, "2024-01-15")
    ]
    
    df = spark.createDataFrame(
        test_data,
        ["cliente_id", "valor_venda", "quantidade", "data_venda"]
    )
    
    result = transform_vendas(df)
    
    # Assertions
    assert result.count() == 2, "Deve ter 2 clientes √∫nicos"
    total_c001 = result.filter(col("cliente_id") == "C001") \
        .select("total_vendas").collect()[0][0]
    assert total_c001 == 150.0, f"Total C001 deve ser 150, foi {total_c001}"
    
    print("‚úÖ Testes passaram!")

def main(date_str):
    spark = create_spark_session()
    
    try:
        # Carregamento
        df = load_raw_data(spark, date_str)
        
        # Transforma√ß√£o
        df_transformed = transform_vendas(df)
        
        # Salvamento particionado
        df_transformed.write \
            .mode("overwrite") \
            .partitionBy("cliente_id") \
            .parquet(f"s3a://data-lake-prod/processed/vendas/dt={date_str}/")
        
        print(f"‚úÖ Processamento conclu√≠do: {df_transformed.count()} registros")
        
    except Exception as e:
        print(f"‚ùå Erro: {str(e)}")
        raise
    finally:
        spark.stop()

if __name__ == "__main__":
    if len(sys.argv) > 1:
        main(sys.argv[1])
    else:
        test_transform()
```

---

## üóÑÔ∏è COMPONENTE 4: STORAGE (8h)

### O que voc√™ vai fazer

**Objetivo**: Armazenar dados de forma escal√°vel, segura e econ√¥mica.

### Conceitos-chave

#### 1. Warehouse Selection
- Redshift vs Snowflake vs BigQuery
- Tradeoffs (custo, performance, features)

#### 2. Particionamento Estrat√©gico
- Parti√ß√£o por data (mais comum)
- Parti√ß√£o por geografia/regi√£o
- Compress√£o (Snappy, ZSTD)

#### 3. Backup & Disaster Recovery
- Replica√ß√£o
- Point-in-time recovery
- RTO/RPO targets

### Documenta√ß√£o Oficial

- **AWS Redshift Best Practices**: https://docs.aws.amazon.com/redshift/latest/mgmt/working-with-clusters.html
- **Snowflake Architecture**: https://docs.snowflake.com/en/user-guide/intro-key-concepts.html
- **Google BigQuery**: https://cloud.google.com/bigquery/docs

### Exemplo: Redshift Schema

```sql
-- Schema com particionamento (DISTSTYLE)
CREATE SCHEMA IF NOT EXISTS warehouse_prod;

CREATE TABLE warehouse_prod.vendas (
    venda_id BIGINT NOT NULL,
    cliente_id INT NOT NULL,
    valor_venda DECIMAL(12, 2),
    data_venda DATE NOT NULL,
    regiao VARCHAR(50),
    created_at TIMESTAMP DEFAULT SYSDATE,
    updated_at TIMESTAMP DEFAULT SYSDATE
)
DISTKEY (cliente_id)  -- Distribui√ß√£o por cliente
SORTKEY (data_venda, cliente_id);  -- Sort para performance

-- Encoding para compress√£o
ALTER TABLE warehouse_prod.vendas 
ENCODE ZSTD;

-- An√°lise para otimizar
ANALYZE TABLE warehouse_prod.vendas;

-- Backup autom√°tico
CREATE OR REPLACE PROCEDURE backup_procedure()
LANGUAGE PLPGSQL
AS $$
BEGIN
    RAISE INFO 'Snapshot criado em %', CURRENT_TIMESTAMP;
END;
$$ EXECUTE;
```

---

## üîÑ COMPONENTE 5: DBT TRANSFORMATIONS (24h)

### O que voc√™ vai fazer

**Objetivo**: Transformar dados de forma testada, documentada e versionada.

### Conceitos-chave

#### 1. Layers
- **Staging**: Limpeza e normaliza√ß√£o do raw
- **Intermediate**: L√≥gica de neg√≥cio intermedi√°ria
- **Marts**: Tabelas finais para consumo

#### 2. Tests Completos
- Unique/not null checks
- Referencial integrity
- Custom tests

#### 3. Documenta√ß√£o
- YAML schemas
- Descri√ß√µes de tabelas/colunas

### Documenta√ß√£o Oficial

- **DBT Documentation**: https://docs.getdbt.com/
- **DBT Best Practices**: https://docs.getdbt.com/guides/best-practices
- **DBT Testing**: https://docs.getdbt.com/docs/build/tests

### Exemplo Pr√°tico: DBT Project

```yaml
# dbt_project.yml
name: 'vendas_analytics'
version: '1.0.0'
profile: 'vendas_prod'

model-paths: ["models"]
analysis-paths: ["analysis"]
test-paths: ["tests"]
data-paths: ["data"]

target-path: "target"
clean-targets:
  - "target"
  - "dbt_packages"

vars:
  start_date: '2020-01-01'
  ambiente: 'prod'

require-dbt-version: [">=1.5.0"]
```

```yaml
# models/staging/staging_vendas.yml
version: 2

models:
  - name: stg_vendas
    description: "Limpeza e normaliza√ß√£o de vendas"
    columns:
      - name: venda_id
        description: "PK - Identificador √∫nico"
        tests:
          - unique
          - not_null
      - name: cliente_id
        description: "FK - Refer√™ncia ao cliente"
        tests:
          - not_null
          - relationships:
              to: ref('stg_clientes')
              field: cliente_id
      - name: valor_venda
        description: "Valor da venda em BRL"
        tests:
          - dbt_expectations.expect_column_values_to_be_between:
              min_value: 0
              max_value: 999999.99
```

```sql
-- models/staging/stg_vendas.sql
{{ config(
    materialized='incremental',
    unique_key='venda_id',
    incremental_strategy='merge'
) }}

WITH raw_vendas AS (
    SELECT
        venda_id,
        cliente_id,
        CAST(valor_venda AS DECIMAL(12, 2)) AS valor_venda,
        CAST(data_venda AS DATE) AS data_venda,
        UPPER(TRIM(regiao)) AS regiao,
        created_at
    FROM {{ source('raw', 'vendas') }}
    
    -- Incremental: s√≥ processar dados novos
    {% if execute %}
        WHERE created_at > (SELECT MAX(created_at) FROM {{ this }})
    {% endif %}
)

SELECT * FROM raw_vendas
WHERE valor_venda > 0
```

```sql
-- models/marts/dim_clientes.sql
{{ config(
    materialized='table',
    indexes=[
        {'columns': ['cliente_id']},
        {'columns': ['data_criacao']}
    ]
) }}

WITH staging AS (
    SELECT * FROM {{ ref('stg_clientes') }}
),

with_metrics AS (
    SELECT
        c.cliente_id,
        c.nome,
        c.email,
        COUNT(DISTINCT v.venda_id) AS total_vendas,
        SUM(v.valor_venda) AS lifetime_value,
        MAX(v.data_venda) AS ultima_compra_data
    FROM staging c
    LEFT JOIN {{ ref('fact_vendas') }} v
        ON c.cliente_id = v.cliente_id
    GROUP BY 1, 2, 3
)

SELECT * FROM with_metrics
```

```yaml
# tests/custom_tests/test_vendas_saldo.sql
# Valida√ß√£o customizada: soma de vendas deve bater com fonte
SELECT
    COUNT(*) AS divergencias
FROM {{ ref('fact_vendas') }}
WHERE valor_venda_acumulado != 
    (SELECT SUM(valor_venda) FROM {{ source('raw', 'vendas') }})
HAVING COUNT(*) > 0
```

---

## ‚úÖ COMPONENTE 6: DATA QUALITY (16h)

### O que voc√™ vai fazer

**Objetivo**: Garantir confiabilidade dos dados com valida√ß√µes automatizadas.

### Conceitos-chave

#### 1. Great Expectations Checks
- Schema validation
- Distributional assertions
- Custom expectations

#### 2. Alertas de Anomalias
- Detec√ß√£o de outliers
- Desvios de volume
- Atrasos

#### 3. SLA Compliance Report
- Porcentagem de sucesso
- Tempo de processamento
- Dados missed

### Documenta√ß√£o Oficial

- **Great Expectations**: https://docs.greatexpectations.io/
- **Data Quality Metrics**: https://docs.greatexpectations.io/docs/guides/validation/

### Exemplo Pr√°tico: Great Expectations

```python
# run_ge_checks.py
from great_expectations.dataset import PandasDataset
import great_expectations as ge
from great_expectations.core.batch import RuntimeBatchRequest
import pandas as pd
import logging

logger = logging.getLogger(__name__)

def create_ge_context():
    """Factory para GE context"""
    return ge.get_context()

def build_vendas_checkpoint(context, date_str):
    """Cria checkpoint para valida√ß√£o de vendas"""
    
    suite = context.create_expectation_suite(
        expectation_suite_name="vendas_validation"
    )
    
    # Carrega dados
    batch_request = RuntimeBatchRequest(
        datasource_name="my_postgres",
        data_connector_name="default_runtime_data_connector",
        data_asset_name="vendas",
        runtime_parameters={"query": f"SELECT * FROM vendas WHERE dt='{date_str}'"}
    )
    
    validator = context.get_validator(
        batch_request=batch_request,
        expectation_suite=suite
    )
    
    # Expectativas
    validator.expect_table_row_count_to_be_between(
        min_value=1000,
        max_value=1000000,
        result_format="SUMMARY"
    )
    
    validator.expect_column_values_to_not_be_null(
        column="venda_id"
    )
    
    validator.expect_column_values_to_be_unique(
        column="venda_id"
    )
    
    validator.expect_column_values_to_be_between(
        column="valor_venda",
        min_value=0,
        max_value=1000000
    )
    
    # Custom expectation: valor m√©dio deve estar entre 50-500
    validator.expect_column_mean_to_be_between(
        column="valor_venda",
        min_value=50,
        max_value=500,
        result_format="SUMMARY"
    )
    
    # Salva suite
    validator.save_expectation_suite()
    
    return suite

def run_checkpoint(context, suite_name, date_str):
    """Executa valida√ß√µes"""
    
    checkpoint_config = {
        "name": f"vendas_checkpoint_{date_str}",
        "config_version": 1.0,
        "class_name": "SimpleCheckpoint",
        "validations": [
            {
                "batch_request": {
                    "datasource_name": "my_postgres",
                    "data_connector_name": "default",
                    "data_asset_name": "vendas"
                },
                "expectation_suite_name": suite_name
            }
        ]
    }
    
    checkpoint = context.add_checkpoint(**checkpoint_config)
    result = checkpoint.run()
    
    # An√°lise de resultado
    validation_result = result["run_results"]
    
    stats = {
        "total_expectations": len(validation_result[0]["validation_result"]["results"]),
        "passed": sum(1 for r in validation_result[0]["validation_result"]["results"] if r["success"]),
        "failed": sum(1 for r in validation_result[0]["validation_result"]["results"] if not r["success"]),
        "timestamp": datetime.now().isoformat()
    }
    
    logger.info(f"QA Report: {stats}")
    
    # Alert se falhas cr√≠ticas
    if stats["failed"] > 0:
        send_alert(f"Falhas em valida√ß√£o: {stats}", channel="data-alerts")
        raise Exception("Data quality check failed")
    
    return stats

def main():
    context = create_ge_context()
    date_str = "2024-01-15"
    
    suite = build_vendas_checkpoint(context, date_str)
    stats = run_checkpoint(context, suite.expectation_suite_name, date_str)
    
    print(f"‚úÖ Valida√ß√£o conclu√≠da: {stats['passed']}/{stats['total_expectations']}")

if __name__ == "__main__":
    main()
```

---

## üìö COMPONENTE 7: DOCUMENTA√á√ÉO + DEPLOYMENT (8h)

### O que voc√™ vai fazer

**Objetivo**: Documentar, deployar e monitorar em produ√ß√£o.

### Deliverables

#### 1. Architecture Diagram
Criado via: Lucidchart, Draw.io, ou Miro
- Mostra todas as camadas
- Data flows
- Servi√ßos externos

#### 2. Runbook para Troubleshooting

```markdown
# Runbook: Pipeline Vendas - Troubleshooting

## Cen√°rio 1: Pipeline falhou na ingest√£o

### Sintomas
- Slack alert: "Task extract_vendas failed"
- UI Airflow: DAG com status FAILED

### Diagn√≥stico
1. Acessar UI Airflow: airflow.empresa.com
2. Navegar para DAG "vendas_ingestao"
3. Clicar em task "extract_vendas" ‚Üí Logs
4. Procurar por:
   - "Connection refused" ‚Üí API offline
   - "401 Unauthorized" ‚Üí Token expirado
   - "Timeout" ‚Üí Rate limiting

### Resolu√ß√£o

#### Se API offline:
- Contactar time de APIs
- Verificar status em status.api.com
- Reexecutar task manualmente: `airflow tasks run vendas_ingestao extract_vendas {{ ds }}`

#### Se token expirado:
- Gerar novo token em portal.api.com
- Atualizar Airflow Variable: `airflow variables set api_token NEW_TOKEN`
- Reexecutar

#### Se timeout:
- Aumentar timeout em DAG: `execution_timeout=timedelta(minutes=15)`
- Verificar rede com: `curl -I https://api.dados.com`
```

#### 3. Cost Analysis

```python
# cost_analysis.py
def calculate_monthly_costs():
    """
    Estimativa de custos por componente
    """
    
    costs = {
        "S3 Storage (1 TB)": 23.00,
        "Airflow Managed (Composer, 16 vCPU)": 3500.00,
        "Spark (10 executors √ó 4GB √ó 730h)": 2000.00,
        "Redshift (dc2.large √ó 2 nodes)": 2000.00,
        "Data Transfer (egress 100GB)": 900.00,
        "Great Expectations": 500.00,
    }
    
    total = sum(costs.values())
    
    # Otimiza√ß√£o: reserved instances salvam 40%
    with_reserved = total * 0.6
    
    print(f"Custo mensal: ${total:,.2f}")
    print(f"Com reserved instances: ${with_reserved:,.2f}")
    print(f"Economia: ${total - with_reserved:,.2f}/m√™s ({40}%)")
    
    return costs
```

---

# üåç EXEMPLOS DO MUNDO REAL

Agora vamos aos casos reais que voc√™ vai enfrentar em produ√ß√£o:

## **Caso 1: E-commerce (Shopify ‚Üí Redshift ‚Üí Tableau)**

### Contexto
Plataforma de e-commerce com 10k+ pedidos/dia, 50+ lojas parceiras.

### Desafios Reais
- Dados chegam de m√∫ltiplas integra√ß√µes (Shopify, WooCommerce, Magento)
- Schema varia entre integra√ß√µes
- Lat√™ncia: relat√≥rios devem estar prontos √†s 06:00

### Arquitetura Implementada

```
Shopify API (webhooks + polling)
    ‚Üì
Lambda + S3 (raw layer, versionado)
    ‚Üì
Airflow (orquestra ingest√£o + processamento)
    ‚Üì
Spark (normaliza schema, limpa dados duplicados)
    ‚Üì
Redshift (Data Warehouse central)
    ‚Üì
DBT (staging ‚Üí marts para diferentes √°reas)
    ‚Üì
Tableau (Analytics dashboard)
```

### M√©tricas de Produ√ß√£o
- Ingest√£o: 50GB/dia
- Lat√™ncia P95: 45 minutos
- Uptime: 99.95%
- Custo mensal: ~$4,500

### Problemas Encontrados

#### 1. Schema evolution
Shopify mudou structure de "customer" 
‚Üí **solu√ß√£o**: UNION ALL com tratamento de novos campos

#### 2. Duplicatas
Webhooks reprocessavam 
‚Üí **solu√ß√£o**: idempot√™ncia com unique_key em DBT

#### 3. Dados incompletos
Pedidos chegavam sem cliente 
‚Üí **solu√ß√£o**: Great Expectations com alertas

### C√≥digo Real: Tratamento de Schema Evolution

```python
# Tratamento de Schema Evolution
from pyspark.sql.types import StructType, StructField, StringType, LongType

def handle_schema_evolution(df_new, schema_expected):
    """Alinha novo schema com esperado"""
    
    for col in schema_expected.fields:
        if col.name not in df_new.columns:
            # Coluna ausente? Adiciona como NULL
            df_new = df_new.withColumn(col.name, lit(None).cast(col.dataType))
    
    # Seleciona apenas colunas esperadas
    df_new = df_new.select([f.name for f in schema_expected.fields])
    
    return df_new
```

---

## **Caso 2: Fintech (Transa√ß√µes em Tempo Real)**

### Contexto
Plataforma de pagamentos com 1M+ transa√ß√µes/dia, requisito de compliance LGPD.

### Desafios Reais
- Dados sens√≠veis (PII): CPF, email, dados banc√°rios
- Auditoria: rastrear TODAS as mudan√ßas
- Performance: queries devem ser <500ms

### Arquitetura

```
Transa√ß√µes (em tempo real via Kafka)
    ‚Üì
Kinesis ‚Üí S3 (com encryption KMS)
    ‚Üì
Glue Catalog (metadata gerenciado)
    ‚Üì
Spark (remove PII, aplica masking)
    ‚Üì
BigQuery (Data Lake)
    ‚Üì
DBT (analytics seguras, sem PII)
    ‚Üì
Data Studio (dashboards)
```

### Seguran√ßa Implementada

```python
from crypto.fernet import Fernet
import re

def mask_pii_data(df, sensitive_columns):
    """Mascara dados sens√≠veis"""
    
    from pyspark.sql.functions import regexp_replace, hash
    
    for col in sensitive_columns:
        if col == 'cpf':
            # Mant√©m √∫ltimos 2 d√≠gitos, mascara resto
            df = df.withColumn(
                col,
                regexp_replace(col, r'(\d{3})\d{5}(\d{2})', 'XXX.XXXX.$2')
            )
        elif col == 'email':
            # firstname***@domain
            df = df.withColumn(
                col,
                regexp_replace(col, r'(.{2}).*(@.+)', '$1***$2')
            )
    
    return df
```

### Auditoria: Change Data Capture

```sql
-- Staging: Versiona TUDO
CREATE TABLE stg_transacoes_history AS
SELECT
    transacao_id,
    usuario_id,
    valor,
    status,
    created_at,
    ROW_NUMBER() OVER (PARTITION BY transacao_id ORDER BY created_at DESC) as versao
FROM raw_transacoes;
```

---

## **Caso 3: SaaS Analytics (Dados de 10k+ clientes)**

### Contexto
Platform que coleta dados de usu√°rios de 10k+ clientes diferentes.

### Desafios
- Isolamento de dados (cada cliente v√™ s√≥ seus dados)
- Escalabilidade: crescer para 100k clientes sem quebrar
- Multi-tenancy: agregar dados mas segregar acesso

### Arquitetura

```
Event tracking (SDK em browser/mobile)
    ‚Üì
Segment/Mixpanel (coleta centralizada)
    ‚Üì
S3 (particionado por tenant_id)
    ‚Üì
Spark (agrega√ß√£o por tenant)
    ‚Üì
Snowflake (Separate Databases por tier de cliente)
    ‚Üì
DBT (materializa√ß√µes por tenant)
    ‚Üì
API REST (serve dados aggregados)
```

### Implementa√ß√£o Multi-tenant

```python
def process_tenant_data(spark, tenant_id, date_str):
    """Processa dados isolados por tenant"""
    
    path = f"s3://events/tenant_id={tenant_id}/dt={date_str}/"
    
    df = spark.read.parquet(path)
    
    # Transforma√ß√µes espec√≠ficas do tenant
    if tenant_id == "cliente_premium":
        # Premium tem acesso a mais eventos
        df = df.filter(col("event_type").isin(PREMIUM_EVENTS))
    else:
        # Free tier: apenas eventos p√∫blicos
        df = df.filter(col("event_type").isin(FREE_EVENTS))
    
    # Salva resultado segregado
    output_path = f"s3://warehouse/tenant={tenant_id}/dt={date_str}/"
    df.write.mode("overwrite").parquet(output_path)
```

---

## **Caso 4: Detec√ß√£o de Fraude (ML + Real-time)**

### Contexto
Bank precisa detectar fraude em tempo real (lat√™ncia <1s).

### Desafios
- Lat√™ncia real-time
- Feature engineering em larga escala
- Modelo deve rodar em produ√ß√£o

### Arquitetura

```
Transa√ß√£o (evento Kafka)
    ‚Üì
Feature Store (Redis cache)
    ‚Üì
ML Model (endpoint prediction)
    ‚Üì
Decision (approve/reject/review)
    ‚Üì
Feedback Loop (atualiza features)
    ‚Üì
Airflow (retreina modelo nightly)
    ‚Üì
BigQuery (audit trail)
```

### Feature Engineering Pipeline

```python
from pyspark.ml import Pipeline
from pyspark.ml.feature import StandardScaler, VectorAssembler

def create_ml_features(df):
    """Feature engineering para modelo de fraude"""
    
    # Temporal features
    df = df.withColumn(
        "hora_transacao",
        hour(col("timestamp"))
    ).withColumn(
        "dias_desde_criacao",
        datediff(col("timestamp"), col("data_criacao"))
    )
    
    # Behavioral features
    window_spec = Window.partitionBy("usuario_id").orderBy("timestamp")
    
    df = df.withColumn(
        "transacoes_ultimas_24h",
        count("*").over(window_spec.rangeBetween(-86400, 0))
    ).withColumn(
        "valor_medio_usuario",
        avg("valor").over(window_spec)
    )
    
    # Feature vector
    feature_cols = ["hora_transacao", "valor", "dias_desde_criacao", 
                    "transacoes_ultimas_24h", "valor_medio_usuario"]
    
    assembler = VectorAssembler(
        inputCols=feature_cols,
        outputCol="features"
    )
    
    scaler = StandardScaler(
        inputCol="features",
        outputCol="scaled_features",
        withMean=True,
        withStd=True
    )
    
    pipeline = Pipeline(stages=[assembler, scaler])
    
    return pipeline.fit(df).transform(df)
```

---

# üìö FONTES COMPLEMENTARES PARA ESTUDO

## Livros & Refer√™ncias
1. **"Fundamentals of Data Engineering"** - Joe Reis & Matt Housley (2022)
2. **"Designing Data-Intensive Applications"** - Martin Kleppmann (imprescind√≠vel)
3. **"The Data Warehouse Toolkit"** - Ralph Kimball (dimensional modeling)

## Plataformas Online
- **Udacity Data Engineering Nanodegree**: https://www.udacity.com/course/data-engineer-nanodegree--nd027
- **DataCamp - Data Engineering Track**: https://www.datacamp.com/courses/data-engineering-track
- **Coursera - Data Engineering on Google Cloud**: https://www.coursera.org/professional-certificates/gcp-data-engineering

## Reposit√≥rios GitHub (estudar c√≥digo real)
- **Uber's Data Engineering Papers**: https://github.com/uber/umap
- **Airflow Examples**: https://github.com/apache/airflow/tree/main/airflow/example_dags
- **DBT Examples**: https://github.com/dbt-labs/dbt-tutorials

## Comunidades & Blogs
- **Data Engineering Weekly**: https://www.dataengineeringweekly.com/
- **Locally Optimistic** (DBT best practices): https://locallyoptimistic.com/
- **Seattle Data Guy** (YouTube): Tutoriais pr√°ticos

---

# ‚úÖ PR√ìXIMOS PASSOS PARA SEU PROJETO

Para alcan√ßar o n√≠vel Pleno, voc√™ precisa:

1. **Semana 1-2**: Montar infra AWS (EC2 para Airflow, RDS para metadata)
2. **Semana 3-4**: Implementar pipeline simples (1 fonte ‚Üí S3 ‚Üí Redshift)
3. **Semana 5-6**: Adicionar Spark e otimiza√ß√µes
4. **Semana 7**: DBT + testes
5. **Semana 8**: Great Expectations + documenta√ß√£o

## Recomenda√ß√£o

Comece com dataset p√∫blico do Kaggle (ex: e-commerce), implemente o pipeline E2E, depois replique em dados reais da empresa.

---

**Documento gerado**: 2025  
**Vers√£o**: 1.0  
**Status**: Pronto para produ√ß√£o
