# ETL IntermediÃ¡rio com Apache Spark - Guia Especialista

OlÃ¡! Com 20 anos em Engenharia de Dados, vou decompor esse programa robusto de aprendizado prÃ¡tico. VocÃª estÃ¡ no caminho certo â€” essa Ã© a estrutura que usamos em produÃ§Ã£o.

## ðŸ“š EXPLICAÃ‡ÃƒO DETALHADA DO PROGRAMA

### 1. EXTRACTION (8h) - FundaÃ§Ãµes SÃ³lidas

VocÃª vai aprender a **ler dados heterogÃªneos** sem quebrar o pipeline:

#### Conceitos-chave:

- **CSV/Parquet/JSON**: Cada formato tem idiossincrasia. CSV pode ter encoding quebrado, Parquet Ã© comprimido (eficiente), JSON Ã© semi-estruturado
- **Data Validation**: Validar schemas evita que dados ruins contaminem transformaÃ§Ãµes
- **Tratamento de CorrupÃ§Ã£o**: Dados do mundo real sÃ£o sujos. UTF-8 com BOM, delimitadores inconsistentes, linhas incompletas

#### DocumentaÃ§Ãµes essenciais:

```
ðŸ“– Apache Spark Documentation:
https://spark.apache.org/docs/latest/sql-data-sources.html

ðŸ“– PySpark SQL Data Sources (CSV, Parquet, JSON):
https://spark.apache.org/docs/latest/sql-data-sources-csv.html
https://spark.apache.org/docs/latest/sql-data-sources-parquet.html
https://spark.apache.org/docs/latest/sql-data-sources-json.html

ðŸ“– Schema Validation & StructType:
https://spark.apache.org/docs/latest/sql-data-types.html
```

#### Exemplo prÃ¡tico que vocÃª farÃ¡:

```python
from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, StringType, IntegerType

spark = SparkSession.builder.appName("ETL_NYC_Taxi").getOrCreate()

# Leitura com schema definido (evita inferÃªncia lenta)
schema = StructType([
    StructField("trip_id", StringType(), True),
    StructField("pickup_datetime", StringType(), True),
    StructField("dropoff_datetime", StringType(), True),
    StructField("trip_distance", IntegerType(), True),
])

df_raw = spark.read \
    .schema(schema) \
    .option("header", "true") \
    .option("badRecordsPath", "/tmp/bad_records") \
    .option("mode", "PERMISSIVE") \
    .csv("s3://datasets/nyc_taxi/2023/")

# ValidaÃ§Ã£o de schema
df_raw.printSchema()
print(f"Total records: {df_raw.count()}")
```

---

### 2. TRANSFORMATION (24h) - O CoraÃ§Ã£o do ETL

Aqui vocÃª entende **como os dados fluem e se transformam**:

#### Data Cleaning (6h):

- Detectar e tratar NULLs estrategicamente
- Remover duplicatas (cuidado: qual coluna define duplicaÃ§Ã£o?)
- Normalizar tipos de dados

#### Feature Engineering (6h):

- Derivar novas colunas que agreguem valor
- Exemplo: de `pickup_datetime` criar `hora_pico`, `dia_semana`, `Ã©_madrugada`

#### AgregaÃ§Ãµes Complexas (6h):

- GROUP BY com mÃºltiplas dimensÃµes
- Window functions (RANK, ROW_NUMBER, LAG/LEAD)
- Percentis e distribuiÃ§Ãµes

#### Joins (4h):

- Inner, Left, Right, Full Outer
- Broadcast joins vs Sort-merge joins (performance!)

#### DocumentaÃ§Ãµes:

```
ðŸ“– PySpark SQL & DataFrame API:
https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/index.html

ðŸ“– Window Functions (crÃ­tico!):
https://spark.apache.org/docs/latest/sql-ref-window-functions.html

ðŸ“– Query Optimization:
https://spark.apache.org/docs/latest/sql-performance-tuning.html

ðŸ“– Delta Lake (para transformaÃ§Ãµes incrementais):
https://docs.delta.io/latest/index.html
```

#### Exemplo prÃ¡tico de Feature Engineering + AgregaÃ§Ã£o:

```python
from pyspark.sql.functions import col, from_unixtime, hour, dayofweek, \
    avg, max, min, row_number, dense_rank, lag
from pyspark.sql.window import Window

# ConversÃ£o de timestamp
df_clean = df_raw \
    .withColumn("pickup_ts", from_unixtime(col("pickup_timestamp"))) \
    .withColumn("hora_pickup", hour(col("pickup_ts"))) \
    .withColumn("dia_semana", dayofweek(col("pickup_ts")))

# Feature: "Ã© pico?" (7-10h, 17-20h)
df_clean = df_clean.withColumn(
    "eh_pico",
    ((col("hora_pickup") >= 7) & (col("hora_pickup") <= 10)) |
    ((col("hora_pickup") >= 17) & (col("hora_pickup") <= 20))
)

# AgregaÃ§Ã£o com Window Function
window_spec = Window.partitionBy("vendor_id").orderBy("pickup_ts")

df_agg = df_clean.withColumn(
    "trip_lag_distance",
    lag(col("trip_distance")).over(window_spec)
).withColumn(
    "vendor_rank",
    dense_rank().over(Window.partitionBy("vendor_id").orderBy(col("total_amount").desc()))
)

# AgregaÃ§Ã£o final
resumo = df_agg.groupBy("hora_pickup", "eh_pico") \
    .agg(
        avg(col("total_amount")).alias("ticket_medio"),
        max(col("trip_distance")).alias("max_distancia"),
        min(col("trip_distance")).alias("min_distancia")
    ) \
    .orderBy(col("hora_pickup"))

resumo.show()
```

---

### 3. LOADING (8h) - Armazenamento Eficiente

#### Parquet (CompressÃ£o + Performance):

- Formato columnar â†’ melhor compressÃ£o
- Particionamento reduz leitura

#### Delta Lake (ACID + Versionamento):

- TransaÃ§Ãµes ACID em data lakes
- Time travel (voltar a versÃ£o anterior)
- Schema enforcement

#### DocumentaÃ§Ã£o:

```
ðŸ“– Spark - Writing DataFrames:
https://spark.apache.org/docs/latest/sql-data-sources-load-save-functions.html

ðŸ“– Partitioning Best Practices:
https://spark.apache.org/docs/latest/sql-data-sources-parquet.html#partition-discovery

ðŸ“– Delta Lake - Overview:
https://docs.delta.io/latest/index.html

ðŸ“– Delta Lake - Write & Read:
https://docs.delta.io/latest/quick-start.html
```

#### Exemplo prÃ¡tico:

```python
# Escrever em Parquet particionado
df_agg.write \
    .mode("overwrite") \
    .partitionBy("eh_pico", "hora_pickup") \
    .parquet("s3://data-lake/nyc_taxi/processed/")

# Escrever em Delta Lake (com versionamento)
df_agg.write \
    .mode("overwrite") \
    .format("delta") \
    .option("mergeSchema", "true") \
    .save("s3://data-lake/nyc_taxi/delta/")

# Leitura com time travel
df_v1 = spark.read.format("delta").option("versionAsOf", 0).load("...")
```

---

### 4. DOCUMENTAÃ‡ÃƒO (8h)

CÃ³digo limpo sem documentaÃ§Ã£o Ã© como um carro de luxo sem manual:

```python
"""
ETL Pipeline: NYC Taxi Data Processing
======================================
Objetivo: Processar dados brutos de tÃ¡xis em NYC e gerar 
agregaÃ§Ãµes para anÃ¡lise de padrÃµes de viagem.

TransformaÃ§Ãµes principais:
1. ValidaÃ§Ã£o de schema (rejeita registros malformados)
2. Feature engineering: hora_pico, dia_semana
3. AgregaÃ§Ãµes: ticket mÃ©dio por hora
4. Carregamento em Delta Lake com particionamento

Autor: [seu_nome]
Data: 2025-10-20
VersÃ£o: 1.0
"""
```

---

## ðŸŒ EXEMPLOS DO MUNDO REAL (Engenharia de Dados SÃªnior)

Aqui estÃ£o os **padrÃµes que vocÃª verÃ¡ em produÃ§Ã£o**:

### Caso 1: E-commerce - RecomendaÃ§Ã£o de Produtos

```
EXTRACTION:
â”œâ”€ PostgreSQL (dados de clientes, pedidos)
â”œâ”€ S3 (logs de cliques em tempo real)
â”œâ”€ API (dados de concorrentes)

TRANSFORMATION:
â”œâ”€ Limpeza: remover bots, sessÃµes com 0 segundos
â”œâ”€ Feature engineering:
â”‚  â”œâ”€ "cliente_premium" = gasto_anual > 5k
â”‚  â”œâ”€ "categoria_preferida" = moda do histÃ³rico
â”‚  â””â”€ "recency" = dias desde Ãºltima compra
â”œâ”€ Joins: usuario Ã— produto Ã— categoria Ã— vendedor
â”œâ”€ Janela temporal: Ãºltimos 30/90/365 dias

LOADING:
â””â”€ Delta Lake particionado por data + categoria
   (serve para retrain de modelos ML diÃ¡rios)

IMPACTO: Aumenta cross-sell em 23% (metÃ¡fora, mas realista)
```

### Caso 2: FinTech - DetecÃ§Ã£o de Fraude

```
EXTRACTION (near real-time):
â”œâ”€ Kafka stream: transaÃ§Ãµes em vivo
â”œâ”€ Cassandra: histÃ³rico de comportamento
â””â”€ Redis: Ãºltimas transaÃ§Ãµes (cache quente)

TRANSFORMATION (complexo!):
â”œâ”€ ValidaÃ§Ãµes: 
â”‚  â”œâ”€ Velocidade: mesma conta em cidades diferentes?
â”‚  â””â”€ PadrÃ£o: compra de R$ 0.01 + 99.99 (teste de cartÃ£o)?
â”œâ”€ Feature engineering:
â”‚  â”œâ”€ Desvio padrÃ£o do valor da transaÃ§Ã£o
â”‚  â”œâ”€ Anomalia em hora/localizaÃ§Ã£o
â”‚  â””â”€ Velocidade de transaÃ§Ãµes (â‰¤5min entre elas?)
â”œâ”€ Joins: transacao Ã— usuario Ã— dispositivo Ã— localizaÃ§Ã£o

LOADING:
â””â”€ Parquet em tempo real + Delta para histÃ³rico
   (feedback loop: depois confirmar se foi fraude)

LATÃŠNCIA: < 500ms (criticamente importante!)
```

### Caso 3: SaÃºde - AnÃ¡lise de Pacientes

```
EXTRACTION:
â”œâ”€ HL7 (formato mÃ©dico de dados)
â”œâ”€ DICOM (imagens mÃ©dicas)
â”œâ”€ EHR (Electronic Health Records - ProntuÃ¡rios)
â””â”€ ValidaÃ§Ã£o HIPAA (conformidade)

TRANSFORMATION:
â”œâ”€ AnonimizaÃ§Ã£o: remover PII (Personally Identifiable Info)
â”œâ”€ Feature engineering:
â”‚  â”œâ”€ Ãndice de Comorbidade de Charlson
â”‚  â”œâ”€ ProgressÃ£o de diagnÃ³stico
â”‚  â””â”€ AderÃªncia ao tratamento
â”œâ”€ Temporal: sequÃªncia de eventos mÃ©dicos
â””â”€ Joins complexos: paciente Ã— consultas Ã— exames Ã— prescriÃ§Ãµes

LOADING:
â””â”€ Data warehouse (RedShift, BigQuery)
   com acesso restrito a roles especÃ­ficas

DESAFIO: Dados altamente sensÃ­veis + conformidade regulatÃ³ria
```

---

## ðŸ› ï¸ COMO APLICAR ISSO NA PRÃTICA

### Semana 1 - Quinta a Sexta:

```bash
# 1. Setup do ambiente
git clone seu-repo
pip install pyspark delta-spark pandas pytest

# 2. ExploraÃ§Ã£o do NYC Taxi Dataset
spark-shell  # ou jupyter notebook

# 3. Primeira leitura + validaÃ§Ã£o
# Comece com 100MB apenas (desenvolvimento rÃ¡pido)
```

### Semana 2 - Segunda a Quarta:

```bash
# Commit structure (histÃ³rico limpo):
git log --oneline
âœ“ feat: extraction com validaÃ§Ã£o de schema
âœ“ feat: feature engineering - temporal features
âœ“ refactor: otimizaÃ§Ã£o de joins com broadcast
âœ“ test: 3 queries validadas com pytest
```

---

## ðŸ“Š RECURSOS ADICIONAIS PARA APROFUNDAMENTO

### Livros (clÃ¡ssicos):

- *"Fundamentals of Data Engineering"* - Joe Reis & Matt Housley
- *"Designing Data-Intensive Applications"* - Martin Kleppmann

### Cursos/CertificaÃ§Ãµes:

- Databricks Academy: https://academy.databricks.com/
- Spark Fundamentals (gratuito): https://training.databricks.com/

### Comunidades:

- Apache Spark Slack: https://s.apache.org/slack-invite
- Stack Overflow tag `apache-spark`
- GitHub: spark trending repositories

### Ferramentas Relacionadas:

```
ETL Orchestration:    Apache Airflow, Prefect, dbt
Data Quality:         Great Expectations, Soda
Monitoring:           Datadog, New Relic
Lakehouse:            Delta Lake, Iceberg, Hudi
```

---

## âœ… CHECKLIST PARA SEU PROJETO

```
[ ] Extraction
    [ ] Ler 3 formatos diferentes
    [ ] Validar 100% dos schemas
    [ ] Rastrear bad records
    
[ ] Transformation
    [ ] Remover 5+ tipos de anomalias
    [ ] Criar 8+ features derivadas
    [ ] Executar 5+ joins
    [ ] Query time < 2 min
    
[ ] Loading
    [ ] Parquet com 4+ partiÃ§Ãµes
    [ ] Delta Lake com versionamento
    [ ] Testes de integridade (count antes/depois)
    
[ ] GitHub
    [ ] README com instruÃ§Ãµes claras
    [ ] Commits atÃ´micos
    [ ] .gitignore correto (dados nÃ£o versionados)
    [ ] 3 queries de teste com resultados esperados
```

---

VocÃª quer que eu elabore mais em algum tÃ³pico especÃ­fico? Recomendo comeÃ§ar pela **Transformation (24h)** â€” Ã© onde os melhores engenheiros de dados passam o tempo! ðŸš€