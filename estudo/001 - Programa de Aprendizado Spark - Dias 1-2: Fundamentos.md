# Programa de Aprendizado Spark - Dias 1-2: Fundamentos

Com 20 anos em Engenharia de Dados, este guia desmembra conceitos de forma pr√°tica e estruturada.

---

## üìö PARTE 1: EXPLICA√á√ÉO T√âCNICA DOS CONCEITOS

### 1. Arquitetura Spark: Driver, Executors, Tasks (2h)

#### O Modelo Mental

Spark funciona como um maestro orquestrando m√∫ltiplos m√∫sicos. Voc√™ n√£o toca, voc√™ dirige a orquestra.

#### Driver (Maestro)
- Roda no seu computador ou servidor
- Cont√©m o programa principal (`SparkContext`)
- Planeja a execu√ß√£o em DAG (gr√°fico ac√≠clico direcionado)
- Coordena e monitora executors
- Coleta resultados

#### Executors (M√∫sicos)
- Processos separados em m√°quinas do cluster
- Executam tarefas em paralelo
- Cada executor tem cache de mem√≥ria pr√≥prio
- Comunicam resultados ao Driver

#### Tasks (Notas Musicais)
- Unidades m√≠nimas de execu√ß√£o
- Uma tarefa por parti√ß√£o de dados
- Rodam paralelamente em threads do executor

#### Fonte Oficial
- [Spark Architecture - Apache Spark Documentation](https://spark.apache.org/docs/latest/cluster-overview.html)
- Se√ß√£o: "Cluster Mode Overview"

---

### 2. RDDs vs DataFrames vs Datasets (2h)

#### RDD (Resilient Distributed Dataset) - O Veterano

```python
# RDD: abstra√ß√£o baixo n√≠vel, imut√°vel
rdd = sc.textFile("dados.txt") \
    .map(lambda x: x.split(","))
```

- Mais controle, menos otimiza√ß√£o
- Use quando: dados desestruturados, transforma√ß√µes customizadas complexas
- Mais lento que DataFrames

#### DataFrame - O Padr√£o Ouro (95% dos casos)

```python
# DataFrame: estruturado, otimizado
df = spark.read.csv("dados.csv", header=True)
df.filter(df.idade > 25).select("nome", "salario")
```

- Schema definido (colunas, tipos)
- Cat√°logo otimizado (Catalyst Optimizer)
- 100-1000x mais r√°pido que RDD
- SQL integrado

#### Dataset - O Tipo-Seguro (Scala/Java)

```scala
// Dataset (Scala): type-safe
val dataset: Dataset[Pessoa] = df.as[Pessoa]
```

- Compila√ß√£o type-check em Scala/Java
- Python n√£o possui Datasets nativos

#### Compara√ß√£o Real

| Aspecto | RDD | DataFrame | Dataset |
|---------|-----|-----------|---------|
| Otimiza√ß√£o | Manual | Autom√°tica (Catalyst) | Autom√°tica + Type-safe |
| Performance | Lenta | R√°pida | R√°pida |
| Schema | Sem estrutura | Com esquema | Tipado |
| Quando usar | Casos especiais | Maioria (ETL) | Sistemas cr√≠ticos (Scala/Java) |

#### Fonte Oficial
- [RDDs vs DataFrames vs Datasets](https://spark.apache.org/docs/latest/sql-getting-started.html)
- [Spark SQL, DataFrames and Datasets Guide](https://spark.apache.org/docs/latest/sql-programming-guide.html)

---

### 3. Lazy Evaluation e DAG (2h)

#### Lazy Evaluation = A Arte de Adiar o Trabalho

```python
# NADA acontece aqui (transforma√ß√µes s√£o "pregui√ßosas")
df_filtrado = df.filter(df.salario > 5000)
df_select = df_filtrado.select("nome")
df_group = df_select.groupBy("departamento").count()

# AQUI SIM! A√ß√£o dispara a execu√ß√£o
resultado = df_group.collect()  # ‚Üê A√á√ÉO
# ou
df_group.write.parquet("/output")  # ‚Üê A√á√ÉO
```

#### Por que isso importa

A avalia√ß√£o tardia permite que Spark **otimize o plano de execu√ß√£o inteiro** antes de qualquer dado ser processado. √â como escrever um script inteiro antes de execut√°-lo.

#### DAG (Directed Acyclic Graph)

```
Transforma√ß√µes:           
filter ‚Üí select ‚Üí groupBy ‚Üí collect

       [DAG]
       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
       ‚îÇ  Seu Plano de Execu√ß√£o     ‚îÇ
       ‚îÇ  Otimizado Automaticamente ‚îÇ
       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
              ‚Üì
       [Catalyst Optimizer]
              ‚Üì
       [Execu√ß√£o Distribu√≠da]
```

#### A√ß√µes que "disparam" o DAG
- `collect()` - trazer para mem√≥ria
- `write.parquet()` - salvar arquivo
- `count()` - contar registros
- `show()` - exibir
- `take(n)` - pegar N linhas

#### Fonte Oficial
- [Lazy Evaluation & DAG - Spark Documentation](https://spark.apache.org/docs/latest/rdd-programming-guide.html#lazy-evaluation)
- [Understanding Apache Spark Internals](https://spark.apache.org/docs/latest/cluster-overview.html)

---

### 4. Criando DataFrames e Leitura de Dados (4h pr√°tico)

#### Inicializar Spark (primeiro passo SEMPRE)

```python
from pyspark.sql import SparkSession

# Criar sess√£o Spark
spark = SparkSession.builder \
    .appName("MeuApp") \
    .master("local[*]") \
    .getOrCreate()
```

#### Formas de Criar DataFrames

```python
# 1Ô∏è‚É£ Ler CSV
df = spark.read \
    .option("header", "true") \
    .option("inferSchema", "true") \
    .csv("/caminho/dados.csv")

# 2Ô∏è‚É£ Ler Parquet (formato comprimido - preferido em produ√ß√£o)
df = spark.read.parquet("/caminho/dados.parquet")

# 3Ô∏è‚É£ Ler JSON
df = spark.read.json("/caminho/dados.json")

# 4Ô∏è‚É£ Ler do Banco de Dados
df = spark.read \
    .format("jdbc") \
    .option("url", "jdbc:postgresql://localhost:5432/db") \
    .option("dbtable", "tabela") \
    .option("user", "usuario") \
    .option("password", "senha") \
    .load()

# 5Ô∏è‚É£ Criar from scratch (Python list)
dados = [
    ("Alice", 30, 5000),
    ("Bob", 25, 4000),
    ("Carlos", 35, 6000)
]
schema = ["nome", "idade", "salario"]
df = spark.createDataFrame(dados, schema=schema)
```

#### Inspecionar DataFrame

```python
df.show(5)              # Exibir primeiras 5 linhas
df.printSchema()        # Mostrar estrutura
df.describe().show()    # Estat√≠sticas b√°sicas
df.info()               # (pandas-like)
df.count()              # Total de linhas (A√á√ÉO - cuidado!)
```

#### Fonte Oficial
- [Reading Data - Spark SQL Guide](https://spark.apache.org/docs/latest/sql-data-sources.html)
- [Creating DataFrames](https://spark.apache.org/docs/latest/sql-getting-started.html)

---

### 5. Transforma√ß√µes B√°sicas (4h pr√°tico)

#### map() - Transformar cada elemento

```python
# RDD (baixo n√≠vel)
numeros_rdd = sc.parallelize([1, 2, 3, 4, 5])
quadrados = numeros_rdd.map(lambda x: x ** 2)
# Resultado: [1, 4, 9, 16, 25]

# DataFrame (alto n√≠vel - melhor)
from pyspark.sql.functions import col, pow

df = spark.createDataFrame([(1,), (2,), (3,)], schema=["numero"])
df_quadrados = df.withColumn("quadrado", pow(col("numero"), 2))
```

#### filter() - Manter apenas linhas que atendem crit√©rio

```python
# RDD
numeros = sc.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])
pares = numeros.filter(lambda x: x % 2 == 0)
# Resultado: [2, 4, 6, 8, 10]

# DataFrame
df = spark.read.csv("vendas.csv", header=True, inferSchema=True)
df_2024 = df.filter(df.ano == 2024)
df_vendas_altas = df.filter((df.valor > 1000) & (df.status == "conclu√≠do"))
```

#### flatMap() - Map + "achatar" resultado

```python
# RDD
frases = sc.parallelize(["ol√° mundo", "apache spark"])
palavras = frases.flatMap(lambda x: x.split(" "))
# Resultado: ["ol√°", "mundo", "apache", "spark"]

# Use case real: explodir arrays
df = spark.createDataFrame([
    ("Alice", ["python", "java", "scala"]),
    ("Bob", ["go", "rust"])
], schema=["nome", "skills"])

from pyspark.sql.functions import explode

df_explodido = df.select("nome", explode(col("skills")).alias("skill"))
# Resultado:
# Alice  python
# Alice  java
# Alice  scala
# Bob    go
# Bob    rust
```

#### Fonte Oficial
- [Transformations - Spark RDD Programming Guide](https://spark.apache.org/docs/latest/rdd-programming-guide.html#transformations)
- [DataFrame API Reference](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.html)

---

## üéØ TAREFA PR√ÅTICA - DIAS 1-2

Arquivo CSV de 100MB com: `id, nome, departamento, salario, data_contratacao, ativo`

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, year

spark = SparkSession.builder \
    .appName("TarefaDias1-2") \
    .master("local[*]") \
    .config("spark.driver.memory", "4g") \
    .config("spark.executor.memory", "4g") \
    .getOrCreate()

# 1Ô∏è‚É£ LER CSV
df = spark.read \
    .option("header", "true") \
    .option("inferSchema", "true") \
    .csv("dados_100mb.csv")

print("Total de registros:", df.count())
df.printSchema()

# 2Ô∏è‚É£ TRANSFORMA√á√ÉO 1: FILTER (colaboradores ativos com sal√°rio > 5000)
df_filtrado = df.filter((col("ativo") == True) & (col("salario") > 5000))

# 3Ô∏è‚É£ TRANSFORMA√á√ÉO 2: SELECT (selecionar apenas colunas relevantes)
df_select = df_filtrado.select("nome", "departamento", "salario")

# 4Ô∏è‚É£ TRANSFORMA√á√ÉO 3: GROUPBY (agrega√ß√£o por departamento)
resultado = df_select \
    .groupBy("departamento") \
    .agg({
        "salario": "avg",
        "nome": "count"
    }) \
    .withColumnRenamed("avg(salario)", "salario_medio") \
    .withColumnRenamed("count(nome)", "total_colaboradores")

# ‚úÖ A√á√ÉO: Salvar resultado
resultado.write.mode("overwrite").csv("/output/resultado")
resultado.show()
```

**Tempo esperado:** 15-30 segundos em m√°quina com 4 cores (dependendo do I/O)

---

## üåç PARTE 2: EXEMPLOS REAIS DE ENGENHARIA DE DADOS

Casos que voc√™ encontrar√° em produ√ß√£o:

### Caso 1: Pipeline ETL em E-commerce

**Contexto Real:** Empresa processa 500GB de logs di√°rios de vendas

```python
from pyspark.sql.functions import col, when, sum, count, avg, date_format
from datetime import datetime, timedelta

# EXTRACT: Ler dados brutos de m√∫ltiplas fontes
vendas = spark.read.parquet("s3://bucket/vendas/2024-10-19/")
clientes = spark.read.jdbc("postgresql://db:5432", "clientes")
produtos = spark.read.json("s3://bucket/produtos/")

# TRANSFORM: Limpeza e enriquecimento
vendas_limpo = vendas \
    .filter(col("valor") > 0) \
    .dropna(subset=["cliente_id", "produto_id"]) \
    .join(clientes, on="cliente_id") \
    .join(produtos, on="produto_id")

# Criar m√©tricas
metricas_diarias = vendas_limpo \
    .groupBy(
        date_format(col("timestamp"), "yyyy-MM-dd").alias("data"),
        col("categoria")
    ) \
    .agg(
        sum("valor").alias("receita_total"),
        count("*").alias("num_transacoes"),
        avg("valor").alias("ticket_medio")
    ) \
    .filter(col("receita_total") > 1000)

# LOAD: Salvar em formato otimizado
metricas_diarias \
    .repartition(10) \
    .write \
    .mode("overwrite") \
    .partitionBy("data") \
    .parquet("s3://bucket/metricas/vendas/")

# Tamb√©m carregar em Data Warehouse
metricas_diarias.write \
    .format("jdbc") \
    .mode("append") \
    .option("url", "jdbc:postgresql://warehouse:5432/analytics") \
    .option("dbtable", "vendas_diarias") \
    .save()
```

**Por que Spark aqui?**
- 500GB de dados n√£o cabe em mem√≥ria de 1 servidor
- Spark distribui em cluster, reduz 8 horas para 15 minutos

---

### Caso 2: Data Lake com Hist√≥rico (SCD - Slowly Changing Dimensions)

**Contexto:** Rastrear mudan√ßas de clientes ao longo do tempo

```python
from pyspark.sql.functions import col, lit, current_timestamp, max, row_number
from pyspark.sql.window import Window

# Novo dados chegando
novos_clientes = spark.read.csv("clientes_novo.csv", header=True)

# Clientes hist√≥ricos (j√° em produ√ß√£o)
clientes_hist = spark.read.parquet("s3://data-lake/clientes/hist/")

# Merge (SCD Type 2: manter hist√≥rico com datas)
novos_com_flag = novos_clientes \
    .withColumn("data_inicio", current_timestamp()) \
    .withColumn("data_fim", lit(None)) \
    .withColumn("versao", lit(1))

# Identificar mudan√ßas
clientes_expirados = clientes_hist \
    .join(
        novos_com_flag,
        on=["cliente_id"],
        how="inner"
    ) \
    .filter(col("status") != col("status_novo")) \
    .select(
        col("cliente_id"),
        col("data_inicio"),
        current_timestamp().alias("data_fim")
    )

# Combinar hist√≥rico com novo
clientes_consolidado = clientes_hist \
    .filter(~col("cliente_id").isin(clientes_expirados.select("cliente_id"))) \
    .union(novos_com_flag) \
    .union(clientes_expirados)

clientes_consolidado \
    .write \
    .mode("overwrite") \
    .parquet("s3://data-lake/clientes/hist/")
```

**Vantagem:** Auditoria completa - sabe exatamente quando cada mudan√ßa ocorreu

---

### Caso 3: Detec√ß√£o de Fraude em Tempo Real (Streaming)

**Contexto:** Processa transa√ß√µes de cart√£o de cr√©dito em tempo real

```python
from pyspark.sql.functions import col, window, count, stddev

# Ler stream de Kafka
df_stream = spark \
    .readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "kafka:9092") \
    .option("subscribe", "transacoes") \
    .load()

# Parse JSON
from pyspark.sql.types import StructType, StructField, StringType, DoubleType

schema = StructType([
    StructField("usuario_id", StringType()),
    StructField("valor", DoubleType()),
    StructField("timestamp", StringType()),
    StructField("pais", StringType())
])

transacoes = df_stream \
    .select(from_json(col("value").cast("string"), schema).alias("dados")) \
    .select("dados.*")

# Detectar anomalias: 3+ transa√ß√µes em pa√≠ses diferentes em 5 min
anomalias = transacoes \
    .groupBy(
        col("usuario_id"),
        window(col("timestamp"), "5 minutes")
    ) \
    .agg(
        count(col("pais").cast("string")).alias("paises_unicos"),
        count("*").alias("num_transacoes")
    ) \
    .filter(col("paises_unicos") >= 2)  # Fraude prov√°vel

# Salvar alertas
query = anomalias \
    .writeStream \
    .format("console") \
    .outputMode("append") \
    .start()

query.awaitTermination()
```

**Por que Spark Streaming?**
- Processa milh√µes de transa√ß√µes/segundo
- Lat√™ncia < 5 segundos
- Escal√°vel horizontalmente

---

### Caso 4: Machine Learning - Feature Engineering

**Contexto:** Preparar dados para modelo preditivo de churn

```python
from pyspark.sql.functions import col, avg, max, min, datediff, current_date
from pyspark.ml import Pipeline
from pyspark.ml.feature import StandardScaler, VectorAssembler

# 1Ô∏è‚É£ Ler dados brutos
df = spark.read.parquet("s3://data/usuarios/")

# 2Ô∏è‚É£ Feature Engineering
features = df \
    .groupBy("usuario_id") \
    .agg(
        avg("valor_gasto").alias("gastos_medio"),
        max("valor_gasto").alias("gastos_maximo"),
        datediff(current_date(), max("ultima_compra")).alias("dias_inatividade"),
        count("*").alias("num_transacoes")
    ) \
    .filter(col("num_transacoes") > 5)

# 3Ô∏è‚É£ Normalizar features
vector_assembler = VectorAssembler(
    inputCols=["gastos_medio", "gastos_maximo", "dias_inatividade", "num_transacoes"],
    outputCol="features"
)

scaler = StandardScaler(
    inputCol="features",
    outputCol="features_normalizadas"
)

pipeline = Pipeline(stages=[vector_assembler, scaler])
df_processado = pipeline.fit(features).transform(features)

# 4Ô∏è‚É£ Salvar para modelo
df_processado.write.parquet("s3://data/features_ml/")
```

**Ganho:** De 10 horas manuais em Pandas para 2 minutos em Spark (10 milh√µes de usu√°rios)

---

## üìä RESUMO DE FONTES POR T√ìPICO

| T√≥pico | Documenta√ß√£o Oficial | Profundidade |
|--------|----------------------|--------------|
| **Arquitetura** | [cluster-overview.html](https://spark.apache.org/docs/latest/cluster-overview.html) | Fundamental |
| **RDD vs DF** | [sql-programming-guide.html](https://spark.apache.org/docs/latest/sql-programming-guide.html) | Intermedi√°rio |
| **DAG & Lazy** | [rdd-programming-guide.html](https://spark.apache.org/docs/latest/rdd-programming-guide.html#lazy-evaluation) | Avan√ßado |
| **Data Sources** | [sql-data-sources.html](https://spark.apache.org/docs/latest/sql-data-sources.html) | Pr√°tico |
| **API Python** | [pyspark.sql Reference](https://spark.apache.org/docs/latest/api/python/) | Reference |
| **Performance** | [tuning.html](https://spark.apache.org/docs/latest/tuning.html) | Avan√ßado |

---

## ‚úÖ Pr√≥ximos Passos Recomendados

1. **Execute a tarefa pr√°tica** (CSV 100MB) hoje
2. **Estude DAG Visualization** - use `spark.sql.explain("extended")`
3. **Experimente com dados reais** - use datasets p√∫blicos (Kaggle, DataHub)
4. **Configure seu ambiente local** com Docker/conda