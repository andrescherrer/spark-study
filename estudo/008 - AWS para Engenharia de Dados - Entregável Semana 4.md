# SEMANA 3-4: AWS - Guia Completo de Engenharia de Dados

## Ãndice
1. [ExplicaÃ§Ã£o dos Componentes](#explicaÃ§Ã£o-dos-componentes)
2. [DocumentaÃ§Ã£o Oficial](#documentaÃ§Ã£o-oficial)
3. [Exemplos do Mundo Real](#exemplos-do-mundo-real)
4. [Estrutura do RepositÃ³rio](#estrutura-recomendada-do-repositÃ³rio)
5. [Boas PrÃ¡ticas](#boas-prÃ¡ticas-terraform)
6. [PrÃ³ximos Passos](#prÃ³ximos-passos-para-a-semana-3-4)
7. [Recursos Essenciais](#recursos-essenciais)

---

## ExplicaÃ§Ã£o dos Componentes

### A. Infrastructure as Code (IaC) com Terraform

**O que Ã© e por quÃª usar:**

Terraform Ã© uma ferramenta declarativa que descreve sua infraestrutura em cÃ³digo. Em Engenharia de Dados, isso significa: reprodutibilidade, versionamento, e destruir/recriar ambientes em minutos.

**Conceitos fundamentais para dados:**

- **Provider AWS**: AutenticaÃ§Ã£o e regiÃ£o
- **Resources**: EC2, RDS, S3, Lambda, Glue, DMS
- **Variables & Outputs**: ParametrizaÃ§Ã£o de ambientes (dev, staging, prod)
- **State Management**: SincronizaÃ§Ã£o entre seu cÃ³digo e infraestrutura real
- **Modules**: ReutilizaÃ§Ã£o (pipeline de ETL encapsulado)

**Por que vocÃª (PHP developer) vai amar:** Ã‰ como versionamento de banco de dados com migrations, mas para TODA infraestrutura.

---

### B. Diagrama de Arquitetura AWS

Uma arquitetura tÃ­pica de Engenharia de Dados segue este fluxo:

```
[Dados Fonte] â†’ [IngestÃ£o] â†’ [Processing] â†’ [Storage] â†’ [Analytics]
     â†“              â†“             â†“            â†“           â†“
  On-Prem      S3/Kinesis/   EC2/Glue/    S3/RDS/     QuickSight/
  Banco Local   DMS/Lambda    Spark       Redshift     Tableau
```

**Componentes essenciais:**

| Camada | ServiÃ§o AWS | FunÃ§Ã£o |
|--------|-----------|--------|
| **IngestÃ£o** | DMS, Kinesis, Glue Crawlers | Capturar dados de mÃºltiplas fontes |
| **Processing** | Glue ETL, EMR, Lambda | Transformar e limpar dados |
| **Storage** | S3 (Data Lake), RDS, Redshift | Persistir dados processados |
| **OrquestraÃ§Ã£o** | Airflow/MWAA, Step Functions | Agendar e controlar pipelines |
| **Analytics** | Athena, Redshift Spectrum | Consultar dados |

---

### C. Estimativa de Custos AWS

**Modelo de custo em Engenharia de Dados:**

```
Custo Mensal â‰ˆ 
  [S3 Storage] + [Data Transfer] + [Lambda Invocations] 
  + [EC2/EMR Hours] + [Redshift Compute] + [DMS Replication]
```

**Exemplo prÃ¡tico (pipeline tÃ­pico):**

- **S3**: 100 GB = ~$2,30/mÃªs (com intelligent tiering)
- **Glue ETL**: 5 DPU, 10 horas/dia = ~$150/mÃªs
- **Lambda**: 1M invocaÃ§Ãµes = ~$0,20/mÃªs
- **Redshift**: dc2.large, 2 nÃ³s = ~$800/mÃªs
- **Data Transfer**: ~50 GB egress = ~$4,50/mÃªs

**Total estimado: ~$956,50/mÃªs**

---

## DocumentaÃ§Ã£o Oficial

### Terraform & AWS
1. **Terraform AWS Provider Official** â†’ https://registry.terraform.io/providers/hashicorp/aws/latest/docs
2. **AWS Terraform Examples** â†’ https://github.com/hashicorp/terraform-aws-examples
3. **Terraform State Management** â†’ https://www.terraform.io/language/state

### AWS Services para Dados
1. **AWS Glue Documentation** â†’ https://docs.aws.amazon.com/glue/
2. **Amazon S3 Best Practices** â†’ https://docs.aws.amazon.com/AmazonS3/latest/userguide/BestPractices.html
3. **AWS DMS (Database Migration Service)** â†’ https://docs.aws.amazon.com/dms/
4. **Amazon Redshift Architecture** â†’ https://docs.aws.amazon.com/redshift/latest/dg/welcome.html
5. **AWS Pricing Calculator** â†’ https://calculator.aws/

### OrquestraÃ§Ã£o & Pipelines
1. **AWS Managed Workflows for Apache Airflow** â†’ https://docs.aws.amazon.com/mwaa/
2. **AWS Step Functions** â†’ https://docs.aws.amazon.com/step-functions/

---

## Exemplos do Mundo Real

### Exemplo 1: E-commerce Data Pipeline (Real-time Analytics)

**Contexto:** VocÃª Ã© responsÃ¡vel pelos dados de vendas de um marketplace com 1M transaÃ§Ãµes/dia.

**Arquitetura:**

```
Apps (PHP) â†’ [API Logs em S3] â†’ [Kinesis Stream] â†’ [Lambda] 
  â†“                                                    â†“
User Events                                      TransformaÃ§Ã£o
                                                     â†“
                                          [S3 Processed/Parquet]
                                                     â†“
                                     [Athena Queries] + [Redshift]
                                                     â†“
                                          [Tableau Dashboard]
```

**Terraform para este caso:**

Ver arquivo `ecommerce_pipeline_tf` (IaC Terraform completo)

**Custo mensal estimado para este cenÃ¡rio:**
- Kinesis (ON_DEMAND): ~$400/mÃªs
- Lambda (100M invocaÃ§Ãµes): ~$2/mÃªs
- S3 (1TB): ~$23/mÃªs
- Redshift (2 nÃ³s): ~$1,600/mÃªs
- **Total: ~$2,025/mÃªs**

**Pontos crÃ­ticos aprendidos:**

1. **IdempotÃªncia**: Seu Lambda deve ser idempotente (rodar 2x = mesmo resultado)
2. **Particionamento S3**: Use `s3://bucket/year=2024/month=10/day=20/hour=15/` para otimizar queries
3. **State Management**: Terraform mantÃ©m estado em `.tfstate` - use S3 remoto em produÃ§Ã£o!

---

### Exemplo 2: DMS (Database Migration Service) - Real-time Replication

**Contexto:** VocÃª tem um banco MySQL local com 500GB de dados transacionais e precisa replicar em tempo real para Redshift.

**Fluxo:**

```
MySQL (On-Prem) --[CDC: Change Data Capture]--> DMS Instance 
                                                  â†“
                                            AWS Redshift
                                                  â†“
                                            Tableau Reports
```

**Terraform DMS:**

```hcl
# DMS Replication Instance
resource "aws_dms_replication_subnet_group" "example" {
  replication_subnet_group_class   = "dms.t3.medium"
  replication_subnet_group_id      = "dms-repinstance-tf"
  replication_subnet_ids           = [aws_subnet.dms.id]
}

resource "aws_dms_replication_instance" "example" {
  replication_instance_id   = "dms-repinstance"
  replication_instance_class = "dms.t3.large"
  allocated_storage          = 100
  engine_version            = "3.4.6"
  subnet_group_id           = aws_dms_replication_subnet_group.example.id
  publicly_accessible       = false
  
  tags = {
    Name = "dms-replication"
  }
}

# Source Endpoint (MySQL On-Prem)
resource "aws_dms_endpoint" "source" {
  endpoint_type               = "source"
  engine_name                 = "mysql"
  server_name                 = "mysql.example.com"
  port                        = 3306
  database_name               = "production_db"
  username                    = "dms_user"
  password                    = "SecurePassword123!"
}

# Target Endpoint (Redshift)
resource "aws_dms_endpoint" "target" {
  endpoint_type               = "target"
  engine_name                 = "redshift"
  server_name                 = aws_redshift_cluster.data_warehouse.endpoint
  port                        = 5439
  database_name               = "analytics_db"
  username                    = "admin"
}

# Replication Task
resource "aws_dms_replication_task" "example" {
  replication_instance_arn    = aws_dms_replication_instance.example.arn
  replication_task_id         = "mysql-to-redshift"
  migration_type              = "cdc"  # Change Data Capture
  source_endpoint_arn         = aws_dms_endpoint.source.arn
  target_endpoint_arn         = aws_dms_endpoint.target.arn
  table_mappings              = jsonencode({
    rules = [{
      rule_type   = "selection"
      rule_id     = "1"
      rule_name   = "include-all-tables"
      object_locator = {
        schema_name = "%"
        table_name  = "%"
      }
      rule_action = "include"
    }]
  })
}
```

**Custo mensal:**
- DMS t3.large: ~$450/mÃªs
- Storage: ~$50/mÃªs
- Data transfer: ~$100/mÃªs
- **Total: ~$600/mÃªs**

---

### Exemplo 3: Glue ETL + S3 + Athena (Data Lake Architecture)

**Contexto:** 10 fontes diferentes (CRM, ERP, APIs, CSVs) precisam ser consolidadas em um Data Lake para analytics.

**Fluxo:**

```
[10 Fontes] â†’ [S3 Raw] â†’ [Glue Crawler] â†’ [Glue Catalog] 
                                              â†“
                                        [Glue ETL Jobs]
                                              â†“
                                    [S3 Processed/Parquet]
                                              â†“
                          [Athena SQL Queries] + [QuickSight]
```

**Por que Glue Ã© poderoso:** VocÃª escreve Python/Scala uma vez, e roda massivamente em paralelo (com Apache Spark).

```hcl
# Glue Catalog Database
resource "aws_glue_catalog_database" "data_lake" {
  name = "data_lake_db"
}

# Glue Crawler (Automatically discovers schema)
resource "aws_glue_crawler" "s3_crawler" {
  database_name = aws_glue_catalog_database.data_lake.name
  name          = "s3-data-crawler"
  role          = aws_iam_role.glue_role.arn
  
  s3_target {
    path = "s3://${aws_s3_bucket.data_lake_raw.id}/crm/"
  }
}

# Glue Job (ETL Processing)
resource "aws_glue_job" "etl_job" {
  name     = "crm-to-warehouse"
  role_arn = aws_iam_role.glue_role.arn
  
  command {
    name            = "glueetl"
    script_location = "s3://${aws_s3_bucket.scripts.id}/crm_etl.py"
  }
  
  default_arguments = {
    "--TempDir"             = "s3://${aws_s3_bucket.temp.id}/"
    "--job-bookmark-option" = "job-bookmark-enable"
  }
  
  glue_version = "4.0"
  max_retries  = 1
  timeout      = 2880
}

# Glue Trigger (Schedule daily at 2 AM)
resource "aws_glue_trigger" "daily_etl" {
  name       = "daily-etl-trigger"
  type       = "ScheduleEvent"
  schedule   = "cron(0 2 * * ? *)"
  actions {
    job_name = aws_glue_job.etl_job.name
  }
}
```

**Exemplo de script Glue ETL (Python):**

```python
import sys
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job

args = getResolvedOptions(sys.argv, ['JOB_NAME', 'S3_INPUT', 'S3_OUTPUT'])
sc = SparkContext()
glueContext = GlueContext(sc)
job = Job(glueContext)
job.init(args['JOB_NAME'], args)

# Read from S3
dyf = glueContext.create_dynamic_frame.from_options(
    format_options={"multiline": False},
    connection_type="s3",
    format="json",
    connection_options={"paths": [args['S3_INPUT']]},
    transformation_ctx="source_data"
)

# Apply transformations
filtered_dyf = Filter.apply(frame=dyf, f=lambda x: x["status"] == "active")
mapped_dyf = ApplyMapping.apply(
    frame=filtered_dyf,
    mappings=[
        ("customer_id", "string", "customer_id", "string"),
        ("email", "string", "email", "string"),
        ("created_at", "string", "created_date", "date"),
    ]
)

# Write to Parquet (optimized columnar format)
glueContext.write_dynamic_frame.from_options(
    frame=mapped_dyf,
    connection_type="s3",
    format="parquet",
    connection_options={"path": args['S3_OUTPUT']},
    transformation_ctx="target_data"
)

job.commit()
```

**Custo mensal:**
- Glue ETL (2 DPU, 8 horas/dia): ~$120/mÃªs
- S3 (500GB): ~$11/mÃªs
- Athena (5TB scanned): ~$25/mÃªs
- **Total: ~$156/mÃªs** (MUITO mais barato!)

---

### Exemplo 4: Breakdown de Custos Realista

#### CenÃ¡rio: 500GB dados/mÃªs, 10M eventos/dia, 50 usuÃ¡rios Analytics

**1. INGESTÃƒO & ORQUESTRAÃ‡ÃƒO**

| ServiÃ§o | Uso | Custo/MÃªs |
|---------|-----|-----------|
| DMS Replication | t3.large, 24/7 | $450 |
| Kinesis Streams | ON_DEMAND, 10M events/dia | $420 |
| Lambda | 2B invocaÃ§Ãµes/mÃªs | $40 |
| **Subtotal IngestÃ£o** | | **$910** |

**2. ARMAZENAMENTO**

| ServiÃ§o | Uso | Custo/MÃªs |
|---------|-----|-----------|
| S3 Raw (Bronze) | 500GB | $11.50 |
| S3 Processed (Silver) | 300GB Parquet | $7 |
| S3 Logs & Backups | 100GB | $2.30 |
| **Subtotal Storage** | | **$20.80** |

**3. PROCESSAMENTO & ETL**

| ServiÃ§o | Uso | Custo/MÃªs |
|---------|-----|-----------|
| Glue ETL Jobs | 5 DPU, 20h/dia | $300 |
| Glue Catalog | First 1M = Free, $1/100k acima | $5 |
| EMR (opcional) | m5.xlarge x3, 40h/mÃªs | $120 |
| **Subtotal Processing** | | **$425** |

**4. DATA WAREHOUSE & ANALYTICS**

| ServiÃ§o | Uso | Custo/MÃªs |
|---------|-----|-----------|
| Redshift | 2x dc2.large, 24/7 | $1,600 |
| Redshift Spectrum | 2TB scanned | $20 |
| Athena | 5TB scanned | $25 |
| QuickSight | 50 usuarios | $450 |
| **Subtotal Analytics** | | **$2,095** |

**5. ORQUESTRAÃ‡ÃƒO & CONTROLE**

| ServiÃ§o | Uso | Custo/MÃªs |
|---------|-----|-----------|
| Airflow (MWAA) | 1x worker c5.xlarge | $280 |
| Step Functions | 50k state transitions | $25 |
| **Subtotal OrquestraÃ§Ã£o** | | **$305** |

**6. MONITORAMENTO & SEGURANÃ‡A**

| ServiÃ§o | Uso | Custo/MÃªs |
|---------|-----|-----------|
| CloudWatch Logs | 100GB ingestion | $50 |
| CloudTrail | 50M events | $2 |
| Secrets Manager | 2 secrets | $0.40 |
| **Subtotal Monitoring** | | **$52.40** |

**7. DATA TRANSFER**

| Tipo | Volume | Custo/MÃªs |
|------|--------|-----------|
| EC2 â†’ S3 (within region) | 1TB | $0 |
| Internet egress | 200GB | $18 |
| Cross-region replication | 100GB | $20 |
| **Subtotal Transfer** | | **$38** |

**RESUMO FINANCEIRO**

| Categoria | Custo |
|-----------|-------|
| IngestÃ£o & OrquestraÃ§Ã£o | $910 |
| Armazenamento | $20.80 |
| Processamento | $425 |
| Analytics & BI | $2,095 |
| OrquestraÃ§Ã£o AvanÃ§ada | $305 |
| Monitoramento | $52.40 |
| Data Transfer | $38 |
| **TOTAL MENSAL** | **$3,846.20** |
| **TOTAL ANUAL** | **$46,154.40** |

**OTIMIZAÃ‡Ã•ES PARA REDUZIR CUSTOS**

1. **Redshift (Custo maior)**
   - **Atividade reduzida?** â†’ Use Redshift Spectrum + Athena apenas
   - **Economia:** -$1,600/mÃªs
   - **Pausar clusters fora de horÃ¡rio** â†’ -20-30%

2. **Glue â†’ Spark (EMR)**
   - Glue Ã© mais caro para jobs intensivos
   - EMR spot instances = -70% em compute
   - **Economia:** -$180/mÃªs

3. **S3 Intelligent Tiering**
   - Move dados automÃ¡ticamente entre access tiers
   - **Economia:** -30-40% em storage

4. **Reduzir retenÃ§Ã£ de dados**
   - Manter apenas 90 dias em hot storage
   - Arquivar em Glacier
   - **Economia:** -$10-15/mÃªs em storage

5. **Reserved Capacity (1-3 anos)**
   - Redshift: -50-60% (prepay)
   - **Economia:** -$800-1000/mÃªs

**CUSTOS OCULTOS (NÃ£o aparecem na conta)**

1. **NAT Gateway** (if applicable): $32/mÃªs + $0.045/GB transfer
2. **VPC Endpoints**: $7.20/mÃªs por endpoint
3. **KMS Encryption**: $1/chave/mÃªs + $0.03/10k requests
4. **Backup Storage** (automated): $0.10/GB/mÃªs
5. **Support Plan**: $100-15,000/mÃªs (depending on level)

**RECOMENDAÃ‡Ã•ES**

âœ… **Use Reserved Capacity** para Redshift/RDS (big savings)
âœ… **Implement S3 Lifecycle Policies** (archive old data)
âœ… **Monitor CloudWatch alarms** para overspending
âœ… **Use Compute Savings Plans** para lambda/Glue
âœ… **Considere multi-region** apenas se necessÃ¡rio
âŒ **Evite sobre-provisioning** de recursos
âŒ **NÃ£o deixe instÃ¢ncias idle** rodando

---

## Estrutura Recomendada do RepositÃ³rio

```
ecommerce-data-pipeline/
â”œâ”€â”€ terraform/
â”‚   â”œâ”€â”€ main.tf                 # Resources principais
â”‚   â”œâ”€â”€ variables.tf            # Todas as variables
â”‚   â”œâ”€â”€ outputs.tf              # Outputs
â”‚   â”œâ”€â”€ s3.tf                   # S3 buckets
â”‚   â”œâ”€â”€ redshift.tf             # Redshift cluster
â”‚   â”œâ”€â”€ kinesis.tf              # Kinesis streams
â”‚   â”œâ”€â”€ lambda.tf               # Lambda functions
â”‚   â”œâ”€â”€ iam.tf                  # Roles & Policies
â”‚   â”œâ”€â”€ terraform.tfvars        # Values (ADD TO .gitignore!)
â”‚   â”œâ”€â”€ backend.tf              # Remote state config
â”‚   â””â”€â”€ modules/                # Reusable modules
â”‚       â”œâ”€â”€ glue_etl/           # Glue job module
â”‚       â”œâ”€â”€ redshift/           # Redshift module
â”‚       â””â”€â”€ monitoring/         # CloudWatch alerts
â”œâ”€â”€ lambda/
â”‚   â”œâ”€â”€ etl_processor/
â”‚   â”‚   â”œâ”€â”€ index.py
â”‚   â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”‚   â””â”€â”€ build.sh
â”‚   â””â”€â”€ data_validator/
â”œâ”€â”€ glue_scripts/
â”‚   â”œâ”€â”€ crm_etl.py
â”‚   â”œâ”€â”€ order_etl.py
â”‚   â””â”€â”€ lib/
â”‚       â””â”€â”€ common.py           # Shared functions
â”œâ”€â”€ airflow/
â”‚   â”œâ”€â”€ dags/
â”‚   â”‚   â”œâ”€â”€ daily_pipeline.py
â”‚   â”‚   â””â”€â”€ weekly_sync.py
â”‚   â””â”€â”€ plugins/
â”œâ”€â”€ sql/
â”‚   â”œâ”€â”€ redshift_schemas.sql
â”‚   â””â”€â”€ views/
â”‚       â”œâ”€â”€ customer_360.sql
â”‚       â””â”€â”€ order_summary.sql
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ ARCHITECTURE.md
â”‚   â”œâ”€â”€ DEPLOYMENT.md
â”‚   â””â”€â”€ COST_OPTIMIZATION.md
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt            # Python deps
```

---

## Boas PrÃ¡ticas Terraform

### 1. **Sempre use Remote State**

```hcl
terraform {
  backend "s3" {
    bucket         = "mycompany-terraform-state"
    key            = "prod/data-pipeline/terraform.tfstate"
    region         = "us-east-1"
    encrypt        = true
    dynamodb_table = "terraform-locks"
  }
}
```

### 2. **Separar por Ambientes**

```
terraform/
â”œâ”€â”€ dev/
â”‚   â”œâ”€â”€ main.tf
â”‚   â””â”€â”€ terraform.tfvars
â”œâ”€â”€ staging/
â””â”€â”€ prod/
```

### 3. **Use Modules para ReutilizaÃ§Ã£o**

```hcl
# modules/glue_etl/main.tf
module "customer_etl" {
  source = "./modules/glue_etl"
  
  job_name = "customer-etl"
  input_path = "s3://raw/crm/"
  output_path = "s3://processed/customers/"
}
```

### 4. **Secrets Management (NUNCA hardcode!)**

```hcl
# Use AWS Secrets Manager
resource "aws_secretsmanager_secret" "db_password" {
  name = "${var.project_name}/rds/password"
}

# E no cÃ³digo:
resource "aws_redshift_cluster" "warehouse" {
  master_password = aws_secretsmanager_secret_version.db.secret_string
}
```

### 5. **ValidaÃ§Ã£o & Formato**

```bash
# Antes de commit
terraform fmt -recursive
terraform validate
terraform plan -out=tfplan

# Code review
terraform show tfplan
```

---

## PrÃ³ximos Passos para a Semana 3-4

### Dia 1-2: Fundamentos Terraform
- [ ] Instale Terraform + AWS CLI
- [ ] Leia: https://www.terraform.io/language/syntax/configuration
- [ ] Crie um S3 bucket simples com TF (hands-on!)

### Dia 3-4: AWS Services Deep-dive
- [ ] Estude S3 + Lifecycle policies
- [ ] Entenda Kinesis vs SQS (quando usar cada um)
- [ ] Leia: https://docs.aws.amazon.com/whitepapers/latest/building-data-lakes-on-aws/

### Dia 5: IntegraÃ§Ã£o AWS + Terraform
- [ ] Crie Kinesis + Lambda trigger com TF
- [ ] Teste o pipeline localmente (sam local)

### Dia 6-7: Diagrama + Custos
- [ ] Desenhe sua arquitetura (Lucidchart/Draw.io)
- [ ] Use AWS Pricing Calculator para estimativa real
- [ ] Documente decisÃµes arquiteturais

---

## Recursos Essenciais

### DocumentaÃ§Ã£o
1. **Terraform AWS**: https://registry.terraform.io/providers/hashicorp/aws/latest
2. **AWS Well-Architected Framework**: https://docs.aws.amazon.com/wellarchitected/
3. **Data Lakes on AWS**: https://docs.aws.amazon.com/whitepapers/
4. **AWS Glue Python API**: https://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-python.html

### Ferramentas PrÃ¡ticas
- **AWS CLI**: `pip install awscli`
- **Terraform**: https://www.terraform.io/downloads
- **Draw.io** (diagramas): https://app.diagrams.net
- **AWS Pricing Calculator**: https://calculator.aws/

### Comunidade
- **HashiCorp Learn**: https://learn.hashicorp.com/terraform
- **AWS Data Engineering Path**: https://aws.amazon.com/training/
- **GitHub Awesome Data Engineering**: https://github.com/igorbarinov/awesome-data-engineering

---

## Dica Final (De veterano)

VocÃª vem de PHP, certo? A maior mudanÃ§a mental serÃ¡ passar de **requisiÃ§Ã£o-resposta** para **batch processing + event streams**. 

Em PHP vocÃª pensa:
```
POST /api/order â†’ Processa â†’ JSON Response
```

Em Engenharia de Dados vocÃª pensa:
```
Events â†’ Stream Infinito â†’ Processa em lotes â†’ Armazena â†’ Consulta analÃ­tica
```

O Terraform Ã© seu amigo aqui. Use-o desde o dia 1 - versione TUDO. Isso que diferencia "scripts de data" de "engenharia profissional".

**Sucesso! ğŸš€**