# Apache Airflow: Dynamic DAGs - Guia Completo

**Especialista em Dados com 20 anos de experi√™ncia**

---

## 1. CONCEITOS FUNDAMENTAIS

### Por que Dynamic DAGs?

Em pipelines tradicionais (est√°ticos), voc√™ precisa codificar cada task manualmente. Em cen√°rios reais, voc√™ n√£o sabe quantas tasks precisar√° executar:

- **M√∫ltiplos clientes**: 1000 clientes, 1000 tasks id√™nticas
- **M√∫ltiplas tabelas**: Ingerir 50 tabelas de um banco de dados dinamicamente
- **Processamento paralelo**: Processar parti√ß√µes que variam diariamente

Dynamic DAGs permitem gerar tasks em **tempo de parsing** (quando o DAG √© lido).

---

## 2. OS TR√äS PADR√ïES

### A. GENERATING TASKS DINAMICAMENTE (Loop com Loop)

```python
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime

# ‚ùå ERRADO - Criar tasks em um loop simples
# Isso criaria a mesma task m√∫ltiplas vezes
# for i in range(10):
#     task = PythonOperator(task_id=f'task_{i}', ...)

# ‚úÖ CORRETO - Usar lista de IDs e loop
with DAG(
    'dynamic_tasks_example',
    start_date=datetime(2024, 1, 1),
    schedule_interval='@daily'
) as dag:
    
    # Definir lista de entidades (clientes, tabelas, etc)
    entidades = ['cliente_A', 'cliente_B', 'cliente_C', 
                 'cliente_D', 'cliente_E', 'cliente_F',
                 'cliente_G', 'cliente_H', 'cliente_I', 'cliente_J']
    
    def processar_entidade(entidade_nome):
        print(f"Processando {entidade_nome}")
        # Sua l√≥gica aqui: ingerir dados, validar, transformar
    
    # PADR√ÉO 1: Loop simples (PR√â-AIRFLOW 2.3)
    tasks = []
    for entidade in entidades:
        task = PythonOperator(
            task_id=f'processar_{entidade}',
            python_callable=processar_entidade,
            op_kwargs={'entidade_nome': entidade}
        )
        tasks.append(task)
    
    # Configurar depend√™ncias
    start_task >> tasks >> end_task
```

**IMPORTANTE**: O loop executa durante o **parsing do DAG** (quando Airflow l√™ o arquivo), n√£o em runtime!

---

### B. SUBDAGS (Padr√£o DESCONTINUADO ‚ö†Ô∏è)

```python
from airflow.operators.subdag import SubDagOperator

def subdag_para_cliente(parent_dag_id, child_dag_id, cliente_id):
    """Subdag: mini-DAG dentro de outro DAG"""
    with DAG(
        dag_id=f'{parent_dag_id}.{child_dag_id}',
        start_date=datetime(2024, 1, 1),
    ) as subdag:
        
        task1 = PythonOperator(
            task_id='extrair_dados',
            python_callable=extrair_dados,
            op_kwargs={'cliente': cliente_id}
        )
        
        task2 = PythonOperator(
            task_id='validar_dados',
            python_callable=validar_dados
        )
        
        task1 >> task2
        return subdag

with DAG('main_dag_com_subdags') as dag:
    clientes = ['cliente_A', 'cliente_B', 'cliente_C']
    
    for cliente in clientes:
        subdag_task = SubDagOperator(
            task_id=f'processo_{cliente}',
            subdag=subdag_para_cliente('main_dag_com_subdags', 'processo', cliente)
        )
```

**‚ö†Ô∏è POR QUE EVITAR**: Subdags t√™m problemas de visibilidade, logging e performance. Apache descontinuou em favor de TaskGroups.

---

### C. TASKGROUPS (RECOMENDADO ‚úÖ - Airflow 2.0+)

```python
from airflow.models import DAG
from airflow.operators.python import PythonOperator
from airflow.utils.task_group import TaskGroup
from datetime import datetime

with DAG(
    'dynamic_taskgroups_best_practice',
    start_date=datetime(2024, 1, 1),
    schedule_interval='@daily',
    catchup=False
) as dag:
    
    clientes = ['cliente_A', 'cliente_B', 'cliente_C', 
                'cliente_D', 'cliente_E', 'cliente_F',
                'cliente_G', 'cliente_H', 'cliente_I', 'cliente_J']
    
    def extrair_dados(cliente_id, **context):
        print(f"Extraindo dados de {cliente_id}")
        # L√≥gica: conectar API/BD, extrair dados
        return f"dados_{cliente_id}"
    
    def validar_dados(cliente_id, **context):
        print(f"Validando dados de {cliente_id}")
    
    def carregar_dados(cliente_id, **context):
        print(f"Carregando dados de {cliente_id}")
    
    # TASKGROUP PARA CADA CLIENTE
    for cliente in clientes:
        with TaskGroup(group_id=f'processar_{cliente}') as tg:
            
            extract = PythonOperator(
                task_id='extract',
                python_callable=extrair_dados,
                op_kwargs={'cliente_id': cliente}
            )
            
            validate = PythonOperator(
                task_id='validate',
                python_callable=validar_dados,
                op_kwargs={'cliente_id': cliente}
            )
            
            load = PythonOperator(
                task_id='load',
                python_callable=carregar_dados,
                op_kwargs={'cliente_id': cliente}
            )
            
            # Ordem dentro do TaskGroup
            extract >> validate >> load
    
    # Todas as TaskGroups rodam em paralelo
```

**VANTAGENS do TaskGroup**:
- ‚úÖ Melhor visibilidade na UI
- ‚úÖ Organiza√ß√£o hier√°rquica clara
- ‚úÖ Melhor suporte a XComs
- ‚úÖ Performance superior

---

## 3. COMPARA√á√ÉO VISUAL

```
LOOP SIMPLES:
‚îú‚îÄ‚îÄ task_cliente_A
‚îú‚îÄ‚îÄ task_cliente_B
‚îú‚îÄ‚îÄ task_cliente_C
...

SUBDAG (Descontinuado):
‚îú‚îÄ‚îÄ subdag_cliente_A
‚îÇ   ‚îú‚îÄ‚îÄ extract
‚îÇ   ‚îú‚îÄ‚îÄ validate
‚îÇ   ‚îî‚îÄ‚îÄ load
‚îú‚îÄ‚îÄ subdag_cliente_B
...

TASKGROUP (Recomendado):
‚îú‚îÄ‚îÄ processar_cliente_A
‚îÇ   ‚îú‚îÄ‚îÄ extract
‚îÇ   ‚îú‚îÄ‚îÄ validate
‚îÇ   ‚îî‚îÄ‚îÄ load
‚îú‚îÄ‚îÄ processar_cliente_B
‚îÇ   ‚îú‚îÄ‚îÄ extract
‚îÇ   ‚îú‚îÄ‚îÄ validate
‚îÇ   ‚îî‚îÄ‚îÄ load
```

---

## 4. DOCUMENTA√á√ÉO OFICIAL

| T√≥pico | URL | Notas |
|--------|-----|-------|
| **Dynamic DAGs** | https://airflow.apache.org/docs/apache-airflow/stable/howto/dynamic-dag-generation.html | Guia principal |
| **TaskGroups** | https://airflow.apache.org/docs/apache-airflow/stable/concepts/tasks.html#task-groups | Padr√£o moderno |
| **Subdags (Deprecated)** | https://airflow.apache.org/docs/apache-airflow/stable/concepts/dags.html#subdags | Evitar para novos projetos |
| **XCom (Passar dados entre tasks)** | https://airflow.apache.org/docs/apache-airflow/stable/concepts/xcoms.html | Essencial para dados din√¢micos |
| **Best Practices** | https://airflow.apache.org/docs/apache-airflow/stable/best-practices.html | Leitura obrigat√≥ria |

---

## 5. SOLU√á√ÉO DA TAREFA: DAG COM 10 TASKS DIN√ÇMICAS

```python
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator
from airflow.utils.task_group import TaskGroup
from datetime import datetime, timedelta
import json

# Configura√ß√µes
default_args = {
    'owner': 'data_team',
    'retries': 2,
    'retry_delay': timedelta(minutes=5),
}

with DAG(
    dag_id='dynamic_10_tasks_dag',
    default_args=default_args,
    start_date=datetime(2024, 1, 1),
    schedule_interval='@daily',
    catchup=False,
    description='DAG que cria 10 tasks dinamicamente baseado em lista'
) as dag:
    
    # ============== CONFIGURA√á√ÉO ==============
    # Lista de entidades a processar (10 items)
    entidades = [
        'vendas_norte',
        'vendas_nordeste',
        'vendas_centro_oeste',
        'vendas_sudeste',
        'vendas_sul',
        'marketing_digital',
        'atendimento_cliente',
        'logistica',
        'financeiro',
        'recursos_humanos'
    ]
    
    # ============== FUN√á√ïES ==============
    def extrair_dados(entidade_id, **context):
        """Simula extra√ß√£o de dados de uma fonte"""
        print(f"üîÑ Extraindo dados para: {entidade_id}")
        
        # Simular dados extra√≠dos
        dados = {
            'entidade': entidade_id,
            'registros': 1500,
            'timestamp': context['execution_date'].isoformat()
        }
        
        # Retornar dados (ser√° armazenado em XCom)
        return json.dumps(dados)
    
    def validar_dados(entidade_id, **context):
        """Valida dados extra√≠dos"""
        print(f"‚úì Validando dados para: {entidade_id}")
        
        # Recuperar dados do upstream task
        task_instance = context['task_instance']
        dados_extraidos = task_instance.xcom_pull(
            task_ids=f'processar_{entidade_id}.extract'
        )
        
        print(f"  Dados recebidos: {dados_extraidos}")
        
        # Simular valida√ß√£o
        validacao = {
            'entidade': entidade_id,
            'linhas_validas': 1450,
            'linhas_invalidas': 50,
            'status': 'OK'
        }
        
        return json.dumps(validacao)
    
    def transformar_dados(entidade_id, **context):
        """Transforma dados para formato final"""
        print(f"‚öôÔ∏è Transformando dados para: {entidade_id}")
        
        transformacao = {
            'entidade': entidade_id,
            'tabelas_criadas': 3,
            'status': 'SUCESSO'
        }
        
        return json.dumps(transformacao)
    
    def carregar_dados(entidade_id, **context):
        """Carrega dados em data warehouse"""
        print(f"üíæ Carregando dados para: {entidade_id}")
        
        # Simular inser√ß√£o em banco de dados
        resultado = {
            'entidade': entidade_id,
            'linhas_inseridas': 1450,
            'tempo_ms': 2500,
            'status': 'CARREGADO'
        }
        
        return json.dumps(resultado)
    
    # ============== TASK DE IN√çCIO ==============
    inicio = BashOperator(
        task_id='inicio_pipeline',
        bash_command='echo "Iniciando pipeline de processamento para 10 entidades"'
    )
    
    # ============== CRIA√á√ÉO DIN√ÇMICA DE TASKS ==============
    # Padr√£o: Usar TaskGroup para agrupar tasks relacionadas
    task_groups = []
    
    for entidade in entidades:
        with TaskGroup(group_id=f'processar_{entidade}') as tg:
            
            # Task 1: Extra√ß√£o
            extract = PythonOperator(
                task_id='extract',
                python_callable=extrair_dados,
                op_kwargs={'entidade_id': entidade},
            )
            
            # Task 2: Valida√ß√£o
            validate = PythonOperator(
                task_id='validate',
                python_callable=validar_dados,
                op_kwargs={'entidade_id': entidade},
            )
            
            # Task 3: Transforma√ß√£o
            transform = PythonOperator(
                task_id='transform',
                python_callable=transformar_dados,
                op_kwargs={'entidade_id': entidade},
            )
            
            # Task 4: Carregamento
            load = PythonOperator(
                task_id='load',
                python_callable=carregar_dados,
                op_kwargs={'entidade_id': entidade},
            )
            
            # Depend√™ncias dentro do TaskGroup
            extract >> validate >> transform >> load
            
            task_groups.append(tg)
    
    # ============== TASK DE FIM ==============
    fim = BashOperator(
        task_id='fim_pipeline',
        bash_command='echo "Pipeline finalizado com sucesso"'
    )
    
    # ============== FLUXO PRINCIPAL ==============
    # inicio >> task_groups >> fim
    # (TaskGroups s√£o executadas em paralelo)
    inicio >> task_groups >> fim
```

---

## 6. CASOS DE USO REAIS EM ENGENHARIA DE DADOS

### üìå Caso 1: Ingest√£o Multi-Tabela de Banco de Dados

**Cen√°rio Real**: Voc√™ precisa extrair 50+ tabelas de um PostgreSQL para Data Lake diariamente.

```python
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.utils.task_group import TaskGroup
from sqlalchemy import create_engine, inspect
import pandas as pd

def obter_tabelas_dinamicamente():
    """Consulta quais tabelas existem no BD em tempo de execu√ß√£o"""
    engine = create_engine('postgresql://user:pass@host/dbname')
    inspector = inspect(engine)
    return inspector.get_table_names()  # Retorna lista din√¢mica

with DAG('ingestao_multi_tabela', ...) as dag:
    
    tabelas = obter_tabelas_dinamicamente()  # ‚Üê Executado no parsing
    
    for tabela in tabelas:
        with TaskGroup(f'processar_tabela_{tabela}') as tg:
            
            extract = PythonOperator(
                task_id='extract',
                python_callable=lambda t=tabela: pd.read_sql(
                    f'SELECT * FROM {t}',
                    con=engine
                ).to_parquet(f'/data/raw/{t}.parquet')
            )
            
            # ... validate, transform, load
```

**Impacto Real**: Ao inv√©s de codificar 50 tasks, voc√™ tem 50 geradas automaticamente!

---

### üìå Caso 2: Processamento de M√∫ltiplos Clientes (SaaS)

**Cen√°rio Real**: Plataforma SaaS com 1000+ clientes. Cada um precisa de ETL di√°rio.

```python
from airflow import DAG
from airflow.models import Variable
from airflow.operators.python import PythonOperator
from airflow.utils.task_group import TaskGroup

def obter_clientes_ativos():
    """Busca clientes ativos do banco de metadados"""
    # SELECT id, name FROM customers WHERE status='ACTIVE'
    return ['cliente_001', 'cliente_002', ..., 'cliente_1000']

with DAG('multi_cliente_etl', schedule_interval='@daily') as dag:
    
    clientes = obter_clientes_ativos()
    
    for cliente_id in clientes:
        with TaskGroup(f'cliente_{cliente_id}') as tg:
            
            # Cada cliente tem seu pr√≥prio pipeline ETL isolado
            # Falha de um n√£o afeta outro
            
            extract = PythonOperator(
                task_id='extract',
                python_callable=extrair_dados_cliente,
                op_kwargs={'customer_id': cliente_id}
            )
            # ... mais tasks
```

**Benef√≠cio**: Isolamento de falhas + processamento paralelo = escalabilidade

---

### üìå Caso 3: Ingest√£o de APIs com Pagina√ß√£o Din√¢mica

**Cen√°rio Real**: API com 100+ endpoints, cada um precisando de sincroniza√ß√£o.

```python
import requests
from airflow.utils.task_group import TaskGroup

def descobrir_endpoints():
    """Consulta OpenAPI/Swagger para descobrir endpoints dinamicamente"""
    response = requests.get('https://api.example.com/swagger.json')
    endpoints = [path for path in response.json()['paths'].keys()]
    return endpoints

with DAG('api_dynamic_ingest') as dag:
    
    endpoints = descobrir_endpoints()
    
    for endpoint in endpoints:
        with TaskGroup(f'sync_{endpoint.replace("/", "_")}') as tg:
            
            def ingerir_endpoint(ep_path, page=1, **context):
                """Sincroniza com pagina√ß√£o din√¢mica"""
                all_data = []
                
                while True:
                    resp = requests.get(
                        f'https://api.example.com{ep_path}?page={page}'
                    )
                    
                    if not resp.json():
                        break
                    
                    all_data.extend(resp.json())
                    page += 1
                
                # Salvar em data lake
                pd.DataFrame(all_data).to_parquet(
                    f'/data/apis/{ep_path.replace("/", "_")}.parquet'
                )
            
            ingerir = PythonOperator(
                task_id='ingest',
                python_callable=ingerir_endpoint,
                op_kwargs={'ep_path': endpoint}
            )
```

---

### üìå Caso 4: Processamento de Parti√ß√µes Din√¢micas (Hadoop/S3)

**Cen√°rio Real**: Processar 10000+ arquivos Parquet particionados por data/regi√£o.

```python
from airflow.operators.python import PythonOperator
from airflow.utils.task_group import TaskGroup
import boto3
from datetime import datetime, timedelta

def descobrir_particoes_s3(bucket, prefix):
    """Lista parti√ß√µes de data que n√£o foram processadas ainda"""
    s3 = boto3.client('s3')
    
    objetos = s3.list_objects_v2(Bucket=bucket, Prefix=prefix)
    particoes = set()
    
    for obj in objetos.get('Contents', []):
        # Extrair data da chave: s3://bucket/data/2024/01/15/...
        partes = obj['Key'].split('/')
        data = f"{partes[2]}/{partes[3]}/{partes[4]}"
        particoes.add(data)
    
    return sorted(list(particoes))

with DAG('processar_particoes_s3') as dag:
    
    particoes = descobrir_particoes_s3('meu-bucket', 'raw/vendas/')
    
    for particao in particoes[-7:]:  # √öltimos 7 dias
        with TaskGroup(f'particao_{particao.replace("/", "_")}') as tg:
            
            processar = PythonOperator(
                task_id='process',
                python_callable=lambda p=particao: processar_dados(
                    f's3://meu-bucket/raw/vendas/{p}/'
                )
            )
```

**Realidade**: Com 10 dias de dados √ó m√∫ltiplas regi√µes = 1000+ tasks geradas automaticamente!

---

### üìå Caso 5: Pipeline Parametrizado com Branch Din√¢mica

**Cen√°rio Real**: Diferentes regras de transforma√ß√£o por tipo de cliente.

```python
from airflow.models import Variable
from airflow.operators.python import PythonOperator
from airflow.utils.task_group import TaskGroup
from airflow.operators.dummy import DummyOperator

def obter_configuracao_clientes():
    """Busca mapeamento cliente -> tipo -> regras"""
    return {
        'cliente_001': {'tipo': 'premium', 'schema': 'premium_rules'},
        'cliente_002': {'tipo': 'standard', 'schema': 'standard_rules'},
        'cliente_003': {'tipo': 'enterprise', 'schema': 'enterprise_rules'},
    }

with DAG('pipeline_parametrizado') as dag:
    
    config = obter_configuracao_clientes()
    
    for cliente_id, detalhes in config.items():
        
        with TaskGroup(f'cliente_{cliente_id}') as tg:
            
            extract = PythonOperator(
                task_id='extract',
                python_callable=extrair_dados,
                op_kwargs={'cliente': cliente_id}
            )
            
            # Aplicar regras espec√≠ficas por tipo de cliente
            if detalhes['tipo'] == 'premium':
                transform = PythonOperator(
                    task_id='transform_premium',
                    python_callable=transformar_premium
                )
            elif detalhes['tipo'] == 'standard':
                transform = PythonOperator(
                    task_id='transform_standard',
                    python_callable=transformar_standard
                )
            
            load = PythonOperator(
                task_id='load',
                python_callable=carregar_para_dw,
                op_kwargs={'schema': detalhes['schema']}
            )
            
            extract >> transform >> load
```

---

## 7. PADR√ïES AVAN√áADOS

### Pattern: Usar Vari√°veis do Airflow para Configura√ß√£o

```python
# No arquivo .env ou UI do Airflow:
# CLIENTES_PARA_PROCESSAR = "['cliente_A', 'cliente_B', 'cliente_C']"

from airflow.models import Variable
import json

clientes = json.loads(Variable.get('CLIENTES_PARA_PROCESSAR'))

for cliente in clientes:
    # ... criar tasks
```

### Pattern: XCom para Passar Dados Entre Tasks Din√¢micas

```python
def tarefa_upstream(**context):
    ti = context['task_instance']
    ti.xcom_push(key='dados_processados', value={'resultado': 42})

def tarefa_downstream(**context):
    ti = context['task_instance']
    # Buscar dados de qualquer task do grupo anterior
    dados = ti.xcom_pull(
        task_ids='processar_cliente_A.tarefa_upstream',
        key='dados_processados'
    )
```

---

## 8. ARMADILHAS COMUNS ‚ö†Ô∏è

| Problema | Causa | Solu√ß√£o |
|----------|-------|--------|
| **Tasks n√£o aparecem** | Loop em runtime, n√£o parsing | Mover l√≥gica para fora do DAG |
| **Muitas tasks** | Gera√ß√£o sem limite (10k+) | Usar `max_active_tasks` + scheduler limits |
| **Mem√≥ria explode** | DAG with 100k tasks carregado em mem√≥ria | Usar SerializedDAG + dynamic task mapping |
| **XCom falha** | Tamanho de dados > limite (24KB padr√£o) | Usar backend externo ou S3 XCom |

---

## 9. PR√ìXIMOS PASSOS PARA APROFUNDAMENTO

1. **Dynamic Task Mapping** (Airflow 2.3+) - mais moderno que TaskGroups
   - https://airflow.apache.org/docs/apache-airflow/stable/howto/dynamic-task-mapping.html

2. **Scheduler Tuning** - quando voc√™ tem 10k+ tasks
   - https://airflow.apache.org/docs/apache-airflow/stable/best-practices.html#general-best-practices

3. **Monitoramento & Alertas** - com Prometheus + Grafana
   - https://airflow.apache.org/docs/apache-airflow/stable/logging-monitoring-alerting.html

---

## RESUMO EXECUTIVO

- **Generating Tasks**: Loop simples para gerar N tasks id√™nticas
- **SubDAGs**: EVITAR - padr√£o descontinuado
- **TaskGroups**: RECOMENDADO - melhor performance e visibilidade
- **Casos reais**: Multi-tabela, multi-cliente, APIs, parti√ß√µes S3, pipelines parametrizados
- **Armadilha comum**: Confundir tempo de parsing com tempo de execu√ß√£o

**Pr√≥xima aula**: Dynamic Task Mapping (Airflow 2.3+) para casos ainda mais escal√°veis!