# Dias 3-4: DataFrames e SQL - Guia Especializado

Bom, vejo que voc√™ est√° em um programa estruturado de aprendizado em Engenharia de Dados. Vou quebrar cada t√≥pico com profundidade e realismo de quem trabalha com isso h√° duas d√©cadas.

---

## 1. OPERA√á√ïES COM DataFrames (SELECT, FILTER, GROUPBY, AGG)

### Conceito Essencial

A maioria dos engenheiros de dados come√ßa pensando em DataFrames como "tabelas na mem√≥ria". Na verdade, eles s√£o **abstra√ß√µes distribu√≠das que otimizam execu√ß√£o lazy**. Voc√™ define o plano, o Spark otimiza e executa quando necess√°rio.

**As 4 opera√ß√µes fundamentais:**

- **SELECT**: Proje√ß√£o de colunas (reduz volume de dados)
- **FILTER**: Predicados que eliminam linhas (mais cr√≠tico do que parece para performance)
- **GROUPBY**: Agrega√ß√£o por chaves
- **AGG**: Fun√ß√µes de agrega√ß√£o (sum, avg, max, count, etc.)

### Exemplo Pr√°tico:

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, sum, avg, count

spark = SparkSession.builder.appName("DataFrameOps").getOrCreate()

# Simulando vendas
vendas_df = spark.read.parquet("vendas.parquet")

# SELECT + FILTER
resultado = (vendas_df
    .select("produto_id", "categoria", "valor_venda", "data_venda")  # proje√ß√£o
    .filter(col("data_venda") >= "2024-01-01")  # predicado
    .filter(col("valor_venda") > 0)  # sempre filtrar outliers
)

# GROUPBY + AGG - agrega√ß√£o multi-coluna
resultado_agg = (resultado
    .groupBy("categoria", "produto_id")
    .agg(
        sum("valor_venda").alias("total_vendas"),
        avg("valor_venda").alias("ticket_medio"),
        count("*").alias("quantidade_transacoes")
    )
)
```

**Por que a ordem importa:** SELECT depois FILTER ‚Üí menos dados trafegando entre parti√ß√µes.

### üìö Fontes Oficiais:

- **Apache Spark DataFrame API** (Python): https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/dataframe.html
- **Spark SQL Functions**: https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html
- **Databricks Learning Path**: https://www.databricks.com/discover/pages/getting-started-with-apache-spark

---

## 2. JOINS - O Cora√ß√£o da Engenharia de Dados

### Os 4 Tipos e Quando Usar

**INNER JOIN**: Retorna apenas registros com correspond√™ncia em ambas as tabelas
```
Tabela A: [1, 2, 3]
Tabela B: [2, 3, 4]
Resultado: [2, 3] ‚úì
```

**LEFT JOIN**: Todos de A + correspond√™ncias de B
```
Resultado: [1, 2, 3] (1 com NULL em B)
```

**RIGHT JOIN**: Todos de B + correspond√™ncias de A

**OUTER JOIN**: Todos de A E B (uni√£o completa)

### Implementa√ß√£o Real:

```python
# DataFrames de exemplo
clientes_df = spark.read.table("clientes")  # [cliente_id, nome, segmento]
pedidos_df = spark.read.table("pedidos")    # [pedido_id, cliente_id, valor]
produtos_df = spark.read.table("pedidos_itens") # [pedido_id, produto_id, qty]

# INNER JOIN - clientes com pedidos
resultado = (clientes_df
    .join(pedidos_df, on="cliente_id", how="inner")
    .join(produtos_df, on="pedido_id", how="inner")
)

# LEFT JOIN - todos os clientes, mesmo sem pedidos
resultado_com_nulls = (clientes_df
    .join(pedidos_df, on="cliente_id", how="left")
)
```

### ‚ö†Ô∏è Performance - O Fator Cr√≠tico

```python
from pyspark.sql import broadcast

# ‚ùå LENTO: Join sem broadcast (default shuffle)
resultado_lento = pedidos_df.join(lookup_pequena, "id")

# ‚úÖ R√ÅPIDO: Broadcast para dimens√µes pequenas (<100MB)
resultado_rapido = pedidos_df.join(
    broadcast(lookup_pequena), 
    "id"
)

# üìä Monitorar execu√ß√£o
resultado_rapido.explain(extended=True)
```

**Regra de ouro em Engenharia de Dados:** Se uma tabela cabe em mem√≥ria (< 100MB por executor), use `broadcast()`.

### üìö Fontes Oficiais:

- **Spark SQL Joins Guide**: https://spark.apache.org/docs/latest/sql-ref-syntax-qry-select-join.html
- **Performance Tuning - Join Strategies**: https://spark.apache.org/docs/latest/sql-performance-tuning.html
- **Databricks - Advanced Joins**: https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-qry-select-join.html

---

## 3. WINDOW FUNCTIONS - O Diferencial

### Conceito: Computa√ß√£o em Grupos Ordenados

Window functions permitem c√°lculos **por linha mantendo contexto do grupo**. √â aqui que a maioria dos iniciantes fica confusa.

```python
from pyspark.sql.window import Window
from pyspark.sql.functions import rank, row_number, dense_rank, lag, lead

# Definir "janela" - parti√ß√£o + ordem
window_por_categoria = (
    Window
    .partitionBy("categoria")
    .orderBy(col("vendas_totais").desc())
)

resultado = (vendas_por_produto
    .withColumn("rank", rank().over(window_por_categoria))
    .withColumn("row_num", row_number().over(window_por_categoria))
    .withColumn("dense_rank", dense_rank().over(window_por_categoria))
    .filter(col("rank") <= 3)  # TOP 3 POR CATEGORIA
)
```

### Diferen√ßas Cr√≠ticas:

| Fun√ß√£o | Comportamento | Uso |
|--------|---------------|-----|
| `rank()` | Pula n√∫meros ap√≥s empate (1,1,3) | Quando empates importam |
| `row_number()` | Sequencial, sem pulos (1,2,3) | Ordem absoluta |
| `dense_rank()` | Sem pulos (1,1,2) | Rankings compactos |

```python
# Exemplo pr√°tico: Margem do cliente ANTES do pedido atual
window_cliente_tempo = (
    Window
    .partitionBy("cliente_id")
    .orderBy("data_pedido")
)

resultado = (pedidos_df
    .withColumn(
        "pedido_anterior_valor",
        lag("valor").over(window_cliente_tempo)
    )
    .withColumn(
        "proxima_data_pedido",
        lead("data_pedido").over(window_cliente_tempo)
    )
)
```

### üìö Fontes Oficiais:

- **Spark Window Functions**: https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/window.html
- **SQL Window Functions Reference**: https://spark.apache.org/docs/latest/sql-ref-syntax-qry-select-window.html
- **Databricks - Advanced Analytics with Window Functions**: https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-qry-select-window.html

---

## 4. SPARK SQL QUERIES

### SQL vs DataFrame API

Spark permite **ambos os paradigmas** - escolha sua arma:

```python
# Abordagem 1: DataFrame API (Pythonic)
resultado_api = (spark
    .table("vendas")
    .filter(col("ano") == 2024)
    .groupBy("categoria")
    .agg(sum("valor").alias("total"))
)

# Abordagem 2: SQL (leg√≠vel, optimiz√°vel)
resultado_sql = spark.sql("""
    SELECT 
        categoria,
        SUM(valor) as total
    FROM vendas
    WHERE ano = 2024
    GROUP BY categoria
""")

# S√£o id√™nticos em performance!
resultado_api.explain(extended=True)
resultado_sql.explain(extended=True)
```

### üìö Fontes Oficiais:

- **Spark SQL Guide**: https://spark.apache.org/docs/latest/sql-programming-guide.html
- **SQL Reference**: https://spark.apache.org/docs/latest/sql-ref.html
- **Databricks SQL**: https://docs.databricks.com/en/sql/

---

## üéØ TAREFA DOS DIAS 3-4: 5 Queries Complexas

### Query 1: Top 3 Produtos por Categoria (seu exemplo)

```python
from pyspark.sql.functions import col, row_number
from pyspark.sql.window import Window

window_categoria = (
    Window
    .partitionBy("categoria")
    .orderBy(col("total_vendas").desc())
)

top3_por_categoria = (
    spark.table("vendas_por_produto")
    .withColumn("rank", row_number().over(window_categoria))
    .filter(col("rank") <= 3)
    .select("categoria", "produto", "total_vendas", "rank")
)
```

### Query 2: Clientes com Maior Crescimento YoY

```python
spark.sql("""
    WITH vendas_por_ano AS (
        SELECT 
            cliente_id,
            YEAR(data_venda) as ano,
            SUM(valor) as total_ano
        FROM vendas
        GROUP BY cliente_id, YEAR(data_venda)
    )
    SELECT 
        cliente_id,
        LAG(total_ano) OVER (PARTITION BY cliente_id ORDER BY ano) as vendas_ano_anterior,
        total_ano as vendas_ano_atual,
        ROUND(((total_ano - LAG(total_ano) OVER (PARTITION BY cliente_id ORDER BY ano)) / LAG(total_ano) OVER (PARTITION BY cliente_id ORDER BY ano) * 100), 2) as crescimento_pct
    FROM vendas_por_ano
    WHERE ano IN (2023, 2024)
    ORDER BY crescimento_pct DESC
""")
```

### Query 3: Produtos com Venda Abaixo da M√©dia da Categoria

```python
spark.sql("""
    WITH media_categoria AS (
        SELECT 
            categoria,
            AVG(preco) as preco_medio
        FROM produtos
        GROUP BY categoria
    )
    SELECT 
        p.produto_id,
        p.categoria,
        p.preco,
        m.preco_medio,
        ROUND((p.preco - m.preco_medio), 2) as diferenca
    FROM produtos p
    LEFT JOIN media_categoria m ON p.categoria = m.categoria
    WHERE p.preco < m.preco_medio
    ORDER BY p.categoria, diferenca
""")
```

### Query 4: Sequ√™ncia de Compras de Clientes (Next Product)

```python
from pyspark.sql.functions import lead

window_cliente_seq = (
    Window
    .partitionBy("cliente_id")
    .orderBy("data_pedido")
)

sequencia_compras = (
    spark.table("pedidos")
    .join(spark.table("itens_pedido"), "pedido_id")
    .withColumn("proximo_produto_id", 
        lead("produto_id").over(window_cliente_seq)
    )
    .withColumn("proxima_data_compra",
        lead("data_pedido").over(window_cliente_seq)
    )
    .filter(col("proximo_produto_id").isNotNull())
)
```

### Query 5: Percentil de Clientes por Valor Total Gasto

```python
spark.sql("""
    SELECT 
        cliente_id,
        SUM(valor) as gasto_total,
        PERCENT_RANK() OVER (ORDER BY SUM(valor)) as percentil,
        NTILE(4) OVER (ORDER BY SUM(valor)) as quartil
    FROM vendas
    GROUP BY cliente_id
    ORDER BY gasto_total DESC
""")
```

---

## üåç Exemplos do Mundo Real - Engenharia de Dados

### Cen√°rio 1: E-commerce - Detec√ß√£o de Churn

```python
spark.sql("""
    WITH cliente_atividade AS (
        SELECT 
            cliente_id,
            DATE_FORMAT(data_compra, 'yyyy-MM') as mes,
            COUNT(*) as qtd_compras,
            SUM(valor) as total_mes
        FROM vendas
        GROUP BY cliente_id, DATE_FORMAT(data_compra, 'yyyy-MM')
    ),
    ultima_atividade AS (
        SELECT 
            cliente_id,
            MAX(mes) as ultimo_mes_ativo,
            DATEDIFF(current_date(), MAX(CONCAT(mes, '-01'))) as dias_inativo
        FROM cliente_atividade
        GROUP BY cliente_id
    )
    SELECT 
        c.cliente_id,
        CASE 
            WHEN dias_inativo > 90 THEN 'Churned'
            WHEN dias_inativo > 60 THEN 'Risk'
            ELSE 'Active'
        END as status_cliente,
        dias_inativo,
        LAG(c.total_mes) OVER (PARTITION BY c.cliente_id ORDER BY c.mes) as mes_anterior_gasto
    FROM cliente_atividade c
    INNER JOIN ultima_atividade u ON c.cliente_id = u.cliente_id
    WHERE c.mes = u.ultimo_mes_ativo
""")
```

### Cen√°rio 2: Fraud Detection - Padr√µes Anormais de Compra

```python
spark.sql("""
    WITH estatisticas_cliente AS (
        SELECT 
            cliente_id,
            AVG(valor_transacao) as valor_medio,
            STDDEV(valor_transacao) as desvio_padrao,
            MAX(valor_transacao) as valor_maximo,
            COUNT(*) as qtd_transacoes
        FROM transacoes
        WHERE data_transacao >= DATE_SUB(current_date(), 180)
        GROUP BY cliente_id
    )
    SELECT 
        t.cliente_id,
        t.valor_transacao,
        e.valor_medio,
        e.desvio_padrao,
        ROUND((t.valor_transacao - e.valor_medio) / e.desvio_padrao, 2) as z_score,
        CASE 
            WHEN ABS((t.valor_transacao - e.valor_medio) / e.desvio_padrao) > 3 THEN 'Anomalia Critica'
            WHEN ABS((t.valor_transacao - e.valor_medio) / e.desvio_padrao) > 2 THEN 'Anomalia Moderada'
            ELSE 'Normal'
        END as classificacao_risco
    FROM transacoes t
    INNER JOIN estatisticas_cliente e ON t.cliente_id = e.cliente_id
    WHERE t.data_transacao = current_date()
    AND ABS((t.valor_transacao - e.valor_medio) / e.desvio_padrao) >= 2
""")
```

### Cen√°rio 3: An√°lise de Reten√ß√£o - Cohort Analysis

```python
spark.sql("""
    WITH primeira_compra AS (
        SELECT 
            cliente_id,
            MIN(DATE_FORMAT(data_compra, 'yyyy-MM')) as cohort
        FROM vendas
        GROUP BY cliente_id
    ),
    atividade_cliente AS (
        SELECT 
            f.cliente_id,
            f.cohort,
            DATE_FORMAT(v.data_compra, 'yyyy-MM') as mes_atividade,
            DATEDIFF(
                DATE_FORMAT(v.data_compra, 'yyyy-MM-01'),
                DATE_FORMAT(DATE_ADD(DATE_PARSE(f.cohort, 'yyyy-MM'), 1), 'yyyy-MM-01')
            ) / 30 as meses_desde_primeira_compra
        FROM primeira_compra f
        INNER JOIN vendas v ON f.cliente_id = v.cliente_id
    )
    SELECT 
        cohort,
        meses_desde_primeira_compra,
        COUNT(DISTINCT cliente_id) as clientes_retidos
    FROM atividade_cliente
    GROUP BY cohort, meses_desde_primeira_compra
    ORDER BY cohort, meses_desde_primeira_compra
""")
```

### Cen√°rio 4: Product Analytics - An√°lise de Funnels de Convers√£o

```python
spark.sql("""
    WITH eventos_ordenados AS (
        SELECT 
            usuario_id,
            evento,
            data_hora,
            ROW_NUMBER() OVER (PARTITION BY usuario_id ORDER BY data_hora) as seq,
            LAG(evento) OVER (PARTITION BY usuario_id ORDER BY data_hora) as evento_anterior,
            LEAD(evento) OVER (PARTITION BY usuario_id ORDER BY data_hora) as proximo_evento
        FROM eventos_aplicacao
        WHERE data_hora >= DATE_SUB(current_date(), 30)
    )
    SELECT 
        evento_anterior,
        evento,
        proximo_evento,
        COUNT(DISTINCT usuario_id) as usuarios_passou_por_essa_sequencia,
        COUNT(CASE WHEN proximo_evento IS NOT NULL THEN 1 END) as continuaram_navegando
    FROM eventos_ordenados
    WHERE evento_anterior IS NOT NULL
    GROUP BY evento_anterior, evento, proximo_evento
    ORDER BY usuarios_passou_por_essa_sequencia DESC
""")
```

### Cen√°rio 5: Supply Chain - An√°lise de Delays

```python
spark.sql("""
    WITH tempos_processamento AS (
        SELECT 
            pedido_id,
            data_pedido,
            data_confirmacao,
            data_saida_deposito,
            data_entrega_estimada,
            data_entrega_real,
            DATEDIFF(data_confirmacao, data_pedido) as dias_para_confirmar,
            DATEDIFF(data_saida_deposito, data_confirmacao) as dias_para_processar,
            DATEDIFF(data_entrega_real, data_saida_deposito) as dias_entrega,
            DATEDIFF(data_entrega_real, data_entrega_estimada) as delay_dias
        FROM pedidos
    )
    SELECT 
        DATE_FORMAT(data_pedido, 'yyyy-MM') as mes_pedido,
        ROUND(AVG(dias_para_confirmar), 1) as media_dias_confirmacao,
        ROUND(AVG(dias_para_processar), 1) as media_dias_processamento,
        ROUND(AVG(dias_entrega), 1) as media_dias_entrega,
        ROUND(AVG(delay_dias), 1) as media_delay,
        COUNT(CASE WHEN delay_dias > 0 THEN 1 END) as pedidos_com_delay,
        ROUND(COUNT(CASE WHEN delay_dias > 0 THEN 1 END) * 100.0 / COUNT(*), 2) as pct_atraso
    FROM tempos_processamento
    GROUP BY DATE_FORMAT(data_pedido, 'yyyy-MM')
    ORDER BY mes_pedido DESC
""")
```

---

## üí° Dicas Profissionais Que Levei 20 Anos Aprendendo

1. **Sempre FILTER antes de JOIN**: Reduz dados trafegando entre parti√ß√µes
2. **Use `explain()` religiosamente**: `df.explain(extended=True)` mostra o plano de execu√ß√£o real
3. **Broadcast √© seu amigo**: Dimens√µes < 100MB? Broadcast. Sempre.
4. **Window functions antes de GROUP BY**: Window functions preservam mais contexto
5. **Evite m√∫ltiplos JOINs em dados grandes**: Considere CTEs ou denormaliza√ß√£o incremental
6. **Monitore shuffle operations**: S√£o o gargalo real. Veja no Spark UI
7. **Reparti√ß√£o estrat√©gica**: Particione por colunas de JOIN/FILTER frequentes

---

Desejo estudar junto? Qual desses cen√°rios do mundo real faz mais sentido para seu contexto? Posso detalhar ainda mais qualquer um deles com c√≥digo completo em Spark + otimiza√ß√µes reais.